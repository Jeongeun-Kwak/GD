{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "loved-portugal",
   "metadata": {},
   "source": [
    "# (G12) 번역가는 대화에도 능하다\n",
    "****\n",
    "# Project: 멋진 챗봇 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-description",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 로드\n",
    "```c\n",
    "$ git clone https://github.com/songys/Chatbot_data.git\n",
    "$ mv Chatbot_data ~/aiffel/transformer_chatbot \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "verbal-fiber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 선언\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "driven-stroke",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/aiffel-dj20/aiffel/transformer_chatbot/Chatbot_data/ChatbotData .csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "small-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df['Q'].values.tolist()\n",
    "answers = df['A'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-drinking",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 정제\n",
    "\n",
    "- ```preprocess_sentence()``` 함수\n",
    "> 1. 영문자는 모두 소문자 변환.\n",
    "> 2. 영문자와 한글, 숫자 그리고 주요 특수문자를 제외하곤 정규식을 활용하여 모두 제거."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "active-update",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    new_sen = []    \n",
    "    for i in sentence:\n",
    "        i = i.lower()\n",
    "        i = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣 a-z A-Z?.!,0-9\\\\s]+\", \"\", i)\n",
    "        new_sen.append(i)\n",
    "    return new_sen\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-nursery",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 토큰화\n",
    "\n",
    "- 토큰화에는 KoNLPy의 ```mecab```클래스를 사용.\n",
    "- 아래 조건을 만족하는 ```build_corpus()```함수를 구현.\n",
    "> 1. 소스 문장 데이터와 타겟 문장 데이터를 입력으로 받음.\n",
    "> 2. 데이터를 앞서 정의한 ```preprocess_sentence()```함수로 정제하고, 토큰화.\n",
    "> 3. 토큰화는 전달받은 토크나이즈 함수를 사용. 이번엔 mecab.morphs 함수를 전달.\n",
    "> 4. 토큰의 개수가 일정 길이 이상인 문장은 데이터에서 제외.\n",
    "> 5. 중복되는 문장은 데이터에서 제외합니다. ```소스 : 타겟``` 쌍을 비교하지 않고 소스는 소스대로 타겟은 타겟대로 검사. 중복 쌍이 흐트러지지 않도록 유의.\n",
    "\n",
    "\n",
    "- 구현한 함수를 활용하여 ```questions```와 ```answers```를 각각 ```que_corpus```, ```ans_corpus```에 토큰화하여 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "selected-suggestion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "def build_corpus(src, tgt):\n",
    "    tokenizer = Mecab()\n",
    "     \n",
    "    src_ = preprocess_sentence(src)\n",
    "    tgt_ = preprocess_sentence(tgt)\n",
    "    \n",
    "    que_corpus = []\n",
    "    ans_corpus = []\n",
    "    \n",
    "    for i in src_:\n",
    "        que_corpus.append(tokenizer.morphs(i))\n",
    "    \n",
    "    for i in tgt_: \n",
    "        ans_corpus.append(tokenizer.morphs(i))\n",
    "    \n",
    "    return que_corpus, ans_corpus\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "mathematical-sunset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11823\n",
      "11823\n"
     ]
    }
   ],
   "source": [
    "que_corpus, ans_corpus = build_corpus(questions, answers)\n",
    "\n",
    "print(len(que_corpus))\n",
    "print(len(ans_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "formed-thanks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['12', '시', '땡', '!'], ['1', '지망', '학교', '떨어졌', '어'], ['3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다'], ['3', '박', '4', '일', '정도', '놀', '러', '가', '고', '싶', '다'], ['ppl', '심하', '네']]\n"
     ]
    }
   ],
   "source": [
    "print(que_corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wrong-lender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['하루', '가', '또', '가', '네요', '.'], ['위로', '해', '드립니다', '.'], ['여행', '은', '언제나', '좋', '죠', '.'], ['여행', '은', '언제나', '좋', '죠', '.'], ['눈살', '이', '찌푸려', '지', '죠', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(ans_corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-congress",
   "metadata": {},
   "source": [
    "## Step 4. Augmentation\n",
    "\n",
    "- Lexical Substitution을 실제로 적용해.\n",
    "```c\n",
    "https://drive.google.com/u/0/ucid=0B0ZXk88koS2KbDhXdWg1Q2RydlU&export=download \n",
    "```\n",
    "\n",
    "\n",
    "- 다운로드한 모델을 활용해 데이터를 Augmentation. 앞서 정의한 ```lexical_sub()```함수를 참고.\n",
    "- Augmentation된 ```que_corpus```와 원본```ans_corpus```가 병렬을 이루도록, 이후엔 반대로 원본```que_corpus```와 Augmentation된```ans_corpus```가 병렬을 이루도록 하여 전체 데이터가 원래의 3배가량으로 늘어나도록 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "professional-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "wv = gensim.models.Word2Vec.load('/home/aiffel-dj20/aiffel/transformer_chatbot/Chatbot_data/ko/ko.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "forbidden-victorian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Substitution 구현하기\n",
    "def lexical_sub(sentence, word2vec):\n",
    "    import random\n",
    "\n",
    "    res = []\n",
    "    toks = sentence\n",
    "\n",
    "    try:\n",
    "        _from = random.choice(toks)\n",
    "        _to = word2vec.most_similar(_from)[0][0]\n",
    "\n",
    "    except:   # 단어장에 없는 단어\n",
    "        return None\n",
    "\n",
    "    for tok in toks:\n",
    "        if tok is _from: res.append(_to)\n",
    "        else: res.append(tok)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "worse-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_temp = []\n",
    "tgt_temp = []\n",
    "\n",
    "que_arg = []\n",
    "ans_arg = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "clean-findings",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj20/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(que_corpus)):\n",
    "    que_temp = lexical_sub(que_corpus[i], wv)\n",
    "    ans_temp = lexical_sub(ans_corpus[i], wv)\n",
    "    que_arg.append(que_temp)\n",
    "    ans_arg.append(ans_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "standard-pathology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12', '시', '땡', '!'] ['12', '시가', '땡', '!']\n",
      "0\n",
      "['1', '지망', '학교', '떨어졌', '어'] ['1', '지망', '학교', '떨어졌', '어서']\n",
      "1\n",
      "['3', '박', '4', '일', '놀', '러', '가', '고', '싶', '다'] ['3', '박', '4', '월과', '놀', '러', '가', '고', '싶', '다']\n",
      "2\n",
      "['3', '박', '4', '일', '정도', '놀', '러', '가', '고', '싶', '다'] ['3', '박', '4', '월과', '정도', '놀', '러', '가', '고', '싶', '다']\n",
      "3\n",
      "['ppl', '심하', '네'] ['ppl', '심하', '카나']\n",
      "4\n",
      "['sd', '카드', '망가졌', '어'] None\n",
      "5\n",
      "['sd', '카드', '안', '돼'] None\n",
      "6\n",
      "['sns', '맞', '팔', '왜', '안', '하', '지', 'ㅠㅠ'] ['sns', '맞', '팔', '과연', '안', '하', '지', 'ㅠㅠ']\n",
      "7\n",
      "['sns', '시간', '낭비', '인', '거', '아', '는데', '매일', '하', '는', '중'] ['sns', '시간', '낭비', '인', '거', '아', '으며', '매일', '하', '는', '중']\n",
      "8\n",
      "['sns', '시간', '낭비', '인데', '자꾸', '보', '게', '됨'] None\n",
      "9\n",
      "['sns', '보', '면', '나', '만', '빼', '고', '다', '행복', '해', '보여'] ['sns', '보', '면', '나의', '만', '빼', '고', '다', '행복', '해', '보여']\n",
      "10\n",
      "['가끔', '궁금', '해'] ['이따금', '궁금', '해']\n",
      "11\n",
      "['가끔', '뭐', '하', '는지', '궁금', '해'] ['가끔', '뭐', '하', '는지', '궁금하', '해']\n",
      "12\n",
      "['가끔', '은', '혼자', '인', '게', '좋', '다'] ['가끔', '은', '거기', '인', '게', '좋', '다']\n",
      "13\n",
      "['가난', '한', '자', '의', '설움'] ['가난', '한', '자', '인의', '설움']\n",
      "14\n",
      "['가만', '있', '어도', '땀', '난다'] ['가만', '있', '더라도', '땀', '난다']\n",
      "15\n",
      "['가상', '화폐', '쫄딱', '망함'] ['가상', '통화', '쫄딱', '망함']\n",
      "16\n",
      "['가스', '불', '켜', '고', '나갔', '어'] ['가스', '불', '켜지', '고', '나갔', '어']\n",
      "17\n",
      "['가스', '불', '켜', '놓', '고', '나온', '거', '같', '아'] ['가스', '불', '켜', '놓', '고', '나온', '거', '같', '아서']\n",
      "18\n",
      "['가스', '비', '너무', '많이', '나왔', '다', '.'] ['가스', '비', '너무', '자주', '나왔', '다', '.']\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    if que_corpus[i] != que_arg[i]:\n",
    "        print(que_corpus[i],que_arg[i])\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "first-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_temp = que_arg    + que_corpus + que_corpus\n",
    "tgt_temp = ans_corpus + ans_arg    + ans_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "wired-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus = src_temp\n",
    "ans_corpus = tgt_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "reported-technology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35469\n",
      "35469\n"
     ]
    }
   ],
   "source": [
    "print(len(que_corpus))\n",
    "print(len(ans_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-sending",
   "metadata": {},
   "source": [
    "## Step 5. 데이터 벡터화\n",
    "\n",
    "- 타겟 데이터인 ```ans_corpus```에 ```<start>```토큰과 ```<end>```토큰이 추가되지 않은 상태이니 이를 먼저 해결한 후 벡터화를 진행. \n",
    "- 우리가 구축한 ```ans_corpus```는 list 형태이기 때문에 아주 쉽게 이를 해결함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "protected-fundamental",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"NoneType\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-40bf491ac3c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mque_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mque_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'<start>'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mque_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'<end>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"NoneType\") to list"
     ]
    }
   ],
   "source": [
    "for i in range(len(que_corpus)):\n",
    "    que_corpus[i] = ['<start>'] + que_corpus[i] + ['<end>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-former",
   "metadata": {},
   "source": [
    "****\n",
    "# 루브릭 평가\n",
    "\n",
    "|평가문항|상세기준|\n",
    "|:----|:----|\n",
    "|1. 챗봇 훈련데이터 전처리 과정이 체계적으로 진행되었는가?|챗봇 훈련데이터를 위한 전처리와 augmentation이 적절히 수행되어 3만개 가량의 훈련데이터셋이 구축되었다.|\n",
    "|2. transformer 모델을 활용한 챗봇 모델이 과적합을 피해 안정적으로 훈련되었는가?|과적합을 피할 수 있는 하이퍼파라미터 셋이 적절히 제시되었다.|\n",
    "|3. 챗봇이 사용자의 질문에 그럴듯한 형태로 답하는 사례가 있는가?|주어진 예문을 포함하여 챗봇에 던진 질문에 적절히 답하는 사례가 제출되었다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-gregory",
   "metadata": {},
   "source": [
    "1. 1번은 길이 조정과 벡터화를 제외하곤 주어진 조건에 맞춰서 최대한 결과를 도출하려고 노력했다. 그런데 list라서 lower이 안된다는 문제를 가지고 오랫동안 고민하는 바람에 다른 것을 풀 시간이 없었다. for문을 쓰면 되는 거였다는 걸 깨닫는데는 많은 시간이 걸렸다.\n",
    "\n",
    "\n",
    "2. 트랜스포머 모델은 구현하지 못했다.\n",
    "\n",
    "\n",
    "3. 챗봇을 만들지 못했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-dancing",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-extreme",
   "metadata": {},
   "source": [
    "챗봇을 만들어야 하는데 솔직히 말하면 요즘 노드에 손이 잘 가지 않는다. 그래서 덮어두고 보지 않고 있다. 챗봇을 한다고 말을 했지만 잘 모르겠다. 잘 모르는 상태에서 어려운 노드를 보니 고통스럽고, 그 고통에서 벗어나기 위해 공부를 멀리하고 있는 것 같다. 이 악순환의 고리를 끊어야 할텐데... 아무것도 하기 싫다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-cooperative",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
