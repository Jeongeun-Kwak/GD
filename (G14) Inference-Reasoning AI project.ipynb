{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "awful-invite",
   "metadata": {},
   "source": [
    "# 프로젝트: 한국어 QA 모델 만들기\n",
    "\n",
    "- 아래 데이터셋은 bAbI 데이터셋을 필자가 한국어로 변환한 한국어 버전의 bAbI 데이터셋이다.\n",
    "- 데이터셋에 대한 소개는 다음과 같다.\n",
    "> 1. 형태는 아래와 같이영어 데이터셋과 동일한 형태를 가진다.\n",
    "> ```c\n",
    "1 은경이는 복도로 가버렸습니다.\n",
    "2 필웅이는 화장실로 뛰어갔습니다.\n",
    "3 은경이는 어디야?     복도  1\n",
    "4 수종이는 화장실로 복귀했습니다.\n",
    "5 은경이는 침실로 갔습니다.\n",
    "6 필웅이는 어디야?     화장실 2\n",
    "7 은경이는 복도로 이동했습니다.\n",
    "8 경임이는 부엌으로 뛰어갔습니다.\n",
    "9 경임이는 어디야?     부엌  8\n",
    "10 경임이는 복도로 가버렸습니다.\n",
    "11 은경이는 정원으로 이동했습니다.\n",
    "12 경임이는 어디야?     복도  10\n",
    "13 경임이는 화장실로 복귀했습니다.\n",
    "14 경임이는 부엌으로 갔습니다.\n",
    "15 경임이는 어디야?     부엌  14\n",
    "> ```\n",
    "\n",
    "\n",
    "- 이번 플로젝트에서는 위 한국어 버전의 데이터셋에 대해서 동장하는 QA 모델을 만들어보자.\n",
    "> (주의!) 기존 케라스 공식 문서의 babi_rnn, babi_memn 구현은 파이썬 3.6을 기준으로 하고 있으며, 파이썬 3.7에서는 정상작동 하지 않을 수 있으니 실습 시에 참고바랍니다.\n",
    "\n",
    "\n",
    "## Step 1. 데이터 다운로드\n",
    "\n",
    "- 다운로드 받아서 압축 해제 후 사용한다.\n",
    "- [e-28-korean.zip](https://drive.google.com/file/d/1GWwjTvSolEKGVuXxPsXJeVH8WCvaS-In/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "atlantic-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 도구들 import\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import tarfile\n",
    "from nltk import FreqDist\n",
    "from functools import reduce\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "varying-wiring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경에 맞게 경로 적절히 수정\n",
    "home_dir = os.getenv('HOME')+'/aiffel/babi_memory_net'\n",
    "DATA_DIR = home_dir + '/e-28-korean'\n",
    "\n",
    "# 훈련 데이터와 시험 데이터의 경로 지정\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"qa1_single-supporting-fact_train_kor.txt\")\n",
    "TEST_FILE = os.path.join(DATA_DIR, \"qa1_single-supporting-fact_test_kor.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "rolled-validity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "print(len(TRAIN_FILE))\n",
    "print(len(TEST_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-colon",
   "metadata": {},
   "source": [
    "- 훈련 데이터에서 20개의 문장을 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "defined-compensation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 필웅이는 화장실로 갔습니다.\n",
      "2 은경이는 복도로 이동했습니다.\n",
      "3 필웅이는 어디야? \t화장실\t1\n",
      "4 수종이는 복도로 복귀했습니다.\n",
      "5 경임이는 정원으로 갔습니다.\n",
      "6 수종이는 어디야? \t복도\t4\n",
      "7 은경이는 사무실로 갔습니다.\n",
      "8 경임이는 화장실로 뛰어갔습니다.\n",
      "9 수종이는 어디야? \t복도\t4\n",
      "10 필웅이는 복도로 갔습니다.\n",
      "11 수종이는 사무실로 가버렸습니다.\n",
      "12 수종이는 어디야? \t사무실\t11\n",
      "13 은경이는 정원으로 복귀했습니다.\n",
      "14 은경이는 침실로 갔습니다.\n",
      "15 경임이는 어디야? \t화장실\t8\n",
      "1 경임이는 사무실로 가버렸습니다.\n",
      "2 경임이는 화장실로 이동했습니다.\n",
      "3 경임이는 어디야? \t화장실\t2\n",
      "4 필웅이는 침실로 이동했습니다.\n",
      "5 수종이는 복도로 갔습니다.\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "lines = open(TRAIN_FILE , \"rb\")\n",
    "for line in lines:\n",
    "    line = line.decode(\"utf-8\").strip()\n",
    "    # lno, text = line.split(\" \", 1) # ID와 TEXT 분리\n",
    "    i = i + 1\n",
    "    print(line)\n",
    "    if i == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-diana",
   "metadata": {},
   "source": [
    "# Step 2. 토크나이저 변경하기 (매우 중요!!!)\n",
    "\n",
    "- 영어권 언어는 띄어쓰기만해도 단어들이 잘 분리되지만, 한국어는 그렇지 않다고 앞에서 몇 차례 언급했다.\n",
    "- 한국어 데이터를 사용하여 모델을 구현하는 것만큼 이번에는 형태소 분석기를 사용해서 단어 토큰화를 해보자.\n",
    "\n",
    "\n",
    "- 그런데 형태소 분석기를 사용할 때, 이런 상황에 봉착한다면 어떻게 해야할까?\n",
    "```\n",
    "형태소 분석 입력 : '은경이는 사무실로 갔습니다.'\n",
    "형태소 분석 결과 : ['은', '경이', '는', '사무실', '로', '갔습니다', '.']\n",
    "```\n",
    "\n",
    "\n",
    "- 사실 위 문장에서 '은경이'는 사람 이름이므로 제대로 된 결과를 얻기 위해서는 '은', '경이'와 같이 글자가 분리되는 것이 아니라 '은경이' 또는 최소한 '은경'이라는 단어 토큰을 얻어야만 한다.\n",
    "\n",
    "\n",
    "- 이런 경우에는 형태소 분석기에 사용자 사전을 추가해줄 수 있다.\n",
    "- '은경이'는 하나의 단어이기 때문에 분리하지말라고 형태소 분석기에 알려주는 것이다.\n",
    "\n",
    "\n",
    "- 사용자 사전을 추가하는 방법은 형태소 분석기마다 다소 다른데, 생각보다 복잡한 경우도 많다.\n",
    "- 이번 실습에서는 Customized Konlpy라는 사용자 사전 추가가 매우 쉬운 패키지를 사용하자.\n",
    "\n",
    "\n",
    "## Customized Konlpy 설치 방법\n",
    "\n",
    "- 설치 방법과 사용 방법에 대한 자세한 설명은 아래의 링크를 참고하라.\n",
    "- [Customized Konlpy 사용하기](https://inspiringpeople.github.io/data%20analysis/ckonlpy/)\n",
    "\n",
    "\n",
    "- 가령, 트위터라는 형태소 분석기를 사용한다고 했을 때, '은경이'라는 단어를 사용자 사전에 추가하고 나서 문장을 형태소 분석하려면 어떻게 해야 할까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "formal-ladder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj20/anaconda3/envs/aiffel/lib/python3.7/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['은경이', '는', '사무실', '로', '갔습니다', '.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예시 코드\n",
    "from ckonlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "twitter.add_dictionary('은경이', 'Noun')\n",
    "twitter.morphs('은경이는 사무실로 갔습니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-guitar",
   "metadata": {},
   "source": [
    "- 모든 문장의 주어 부분을 자른 다음에 리스트에 넣고, set()을 사용하면 주어에 어떤 이름이 들어갔는지 볼 수 있지 않을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ranging-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = []\n",
    "temp = []\n",
    "\n",
    "lines = open(TRAIN_FILE , \"rb\")\n",
    "for line in lines:\n",
    "    line = line.decode(\"utf-8\").strip()\n",
    "    lno, text = line.split(\" \", 1) # ID와 TEXT 분리\n",
    "    temp = text[0:3]\n",
    "    subjects.append(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "brazilian-labor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'경임이', '수종이', '은경이', '필웅이'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "unlikely-fellowship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['경임이', '는', '사무실', '로', '가버렸습니다', '.']\n",
      "['수종이', '는', '복도', '로', '갔습니다', '.']\n",
      "['은경이', '는', '사무실', '로', '갔습니다', '.']\n",
      "['필웅이', '는', '어디', '야', '?']\n"
     ]
    }
   ],
   "source": [
    "twitter.add_dictionary('경임이', 'Noun')\n",
    "print(twitter.morphs('경임이는 사무실로 가버렸습니다.'))\n",
    "\n",
    "twitter.add_dictionary('수종이', 'Noun')\n",
    "print(twitter.morphs('수종이는 복도로 갔습니다.'))\n",
    "\n",
    "twitter.add_dictionary('은경이', 'Noun')\n",
    "print(twitter.morphs('은경이는 사무실로 갔습니다.'))\n",
    "\n",
    "twitter.add_dictionary('필웅이', 'Noun')\n",
    "print(twitter.morphs('필웅이는 어디야?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "forward-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dir):\n",
    "    stories, questions, answers = [], [], [] # 각각 스토리, 질문, 답변을 저장할 예정\n",
    "    story_temp = [] # 현재 시점의 스토리 임시 저장\n",
    "    lines = open(dir, \"rb\")\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.decode(\"utf-8\") # b' 제거\n",
    "        line = line.strip() # '\\n' 제거\n",
    "        idx, text = line.split(\" \", 1) # 맨 앞에 있는 id number 분리\n",
    "        # 여기까지는 모든 줄에 적용되는 전처리\n",
    "\n",
    "        if int(idx) == 1:\n",
    "            story_temp = []\n",
    "        \n",
    "        if \"\\t\" in text: # 현재 읽는 줄이 질문 (tab) 답변 (tab)인 경우\n",
    "            question, answer, _ = text.split(\"\\t\") # 질문과 답변을 각각 저장\n",
    "            stories.append([x for x in story_temp if x]) # 지금까지의 누적 스토리를 스토리에 저장\n",
    "            questions.append(question)\n",
    "            answers.append(answer)\n",
    "\n",
    "        else: # 현재 읽는 줄이 스토리인 경우\n",
    "            story_temp.append(text) # 임시 저장\n",
    "\n",
    "    lines.close()\n",
    "    return stories, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "sexual-outreach",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_data(TRAIN_FILE)\n",
    "test_data = read_data(TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "secure-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stories, train_questions, train_answers = read_data(TRAIN_FILE)\n",
    "test_stories, test_questions, test_answers = read_data(TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dietary-captain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 스토리 개수: 10000\n",
      "train 질문 개수: 10000\n",
      "train 답변 개수: 10000\n",
      "test 스토리 개수: 1000\n",
      "test 질문 개수: 1000\n",
      "test 답변 개수: 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"train 스토리 개수:\", len(train_stories))\n",
    "print(\"train 질문 개수:\", len(train_questions))\n",
    "print(\"train 답변 개수:\", len(train_answers))\n",
    "print(\"test 스토리 개수:\", len(test_stories))\n",
    "print(\"test 질문 개수:\", len(test_questions))\n",
    "print(\"test 답변 개수:\", len(test_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "working-iraqi",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['수종이는 화장실로 뛰어갔습니다.',\n",
       " '경임이는 사무실로 갔습니다.',\n",
       " '은경이는 부엌으로 이동했습니다.',\n",
       " '경임이는 화장실로 갔습니다.',\n",
       " '수종이는 복도로 갔습니다.',\n",
       " '필웅이는 부엌으로 가버렸습니다.',\n",
       " '수종이는 부엌으로 이동했습니다.',\n",
       " '수종이는 화장실로 가버렸습니다.',\n",
       " '필웅이는 사무실로 갔습니다.',\n",
       " '수종이는 사무실로 갔습니다.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3879번째 스토리 출력\n",
    "train_stories[3879]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "packed-campus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['필웅이는 어디야? ', '수종이는 어디야? ', '수종이는 어디야? ', '수종이는 어디야? ', '경임이는 어디야? ']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상위 5개 질문 출력\n",
    "train_questions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "sacred-therapist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['화장실', '복도', '복도', '사무실', '화장실']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상위 5개 답변 출력\n",
    "train_answers[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-creator",
   "metadata": {},
   "source": [
    "## Step 2. 형태소 분석 후 불용어 처리하기\n",
    "- 형태소 분석기로 문자를 분석해보면 '는', '으로' 등 분석에 크게 도움이 되지 않을 것 같은 토큰들이 나올 것이다. \n",
    "- 처음에는 이 토큰들도 그대로 사용해서 모델을 구현해보고, 두 번째 구현에서는 이 토큰들을 전처리 과정에서 불용어 처리하여 제외해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-documentation",
   "metadata": {},
   "source": [
    "## 불용어 제거하지 않고 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "sunset-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    return [ x.strip() for x in re.sub(r\"\\s+|\\b\", '\\f', sent).split('\\f') if x.strip() ] # python 3.7의 경우 \n",
    "    # return [ x.strip() for x in re.split('(\\W+)?', sent) if x.strip()] # python 3.6의 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "crazy-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data):\n",
    "    counter = FreqDist()\n",
    "    \n",
    "    # 두 문장의 story를 하나의 문장으로 통합하는 함수\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "\n",
    "    # 각 샘플의 길이를 저장하는 리스트\n",
    "    story_len = []\n",
    "    question_len = []\n",
    "    \n",
    "    for stories, questions, answers in [train_data, test_data]:\n",
    "        for story in stories:\n",
    "            stories = tokenize(flatten(story)) # 스토리의 문장들을 펼친 후 토큰화\n",
    "            story_len.append(len(stories)) # 각 story의 길이 저장\n",
    "            \n",
    "            for word in stories: # 단어 집합에 단어 추가\n",
    "                counter[word] += 1\n",
    "        \n",
    "        for question in questions:\n",
    "            question = tokenize(question)\n",
    "            question_len.append(len(question))\n",
    "            \n",
    "            for word in question:\n",
    "                counter[word] += 1\n",
    "        \n",
    "        for answer in answers:\n",
    "            answer = tokenize(answer)\n",
    "            \n",
    "            for word in answer:\n",
    "                counter[word] += 1\n",
    "\n",
    "    # 단어장 생성\n",
    "    word2idx = {word : (idx + 1) for idx, (word, _) in enumerate(counter.most_common())}\n",
    "    idx2word = {idx : word for word, idx in word2idx.items()}\n",
    "\n",
    "    # 가장 긴 샘플의 길이\n",
    "    story_max_len = np.max(story_len)\n",
    "    question_max_len = np.max(question_len)\n",
    "\n",
    "    return word2idx, idx2word, story_max_len, question_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "induced-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx, idx2word, story_max_len, question_max_len = preprocess_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "civilian-documentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 1, '경임이는': 2, '은경이는': 3, '수종이는': 4, '필웅이는': 5, '이동했습니다': 6, '가버렸습니다': 7, '뛰어갔습니다': 8, '복귀했습니다': 9, '갔습니다': 10, '화장실로': 11, '정원으로': 12, '복도로': 13, '어디야': 14, '?': 15, '부엌으로': 16, '사무실로': 17, '침실로': 18, '화장실': 19, '정원': 20, '사무실': 21, '침실': 22, '복도': 23, '부엌': 24}\n"
     ]
    }
   ],
   "source": [
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "necessary-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "settled-greeting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스토리의 최대 길이 : 40\n",
      "질문의 최대 길이 : 3\n"
     ]
    }
   ],
   "source": [
    "print('스토리의 최대 길이 :',story_max_len)\n",
    "print('질문의 최대 길이 :',question_max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-closer",
   "metadata": {},
   "source": [
    "## Step 3. 한국어에서의 모델 정확도 확인해보기\n",
    "\n",
    "- 앞서 만든 메모리 네트워크는 영어권 데이터에서는 보편적으로 약 96% 이상의 높은 성능을 보인다.\n",
    "- 하이퍼파라미터를 잘 조정하면 이보다 더 높은 정확도가 나오기도 한다.\n",
    "- 그렇다면 메모리 네트워크가 한국어에서도 영어만큼 잘 동작하는지 직접 확인해보자.\n",
    "\n",
    "\n",
    "## 1. 불용어를 제거하지 않은 데이터를 사용한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "mobile-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data, word2idx, story_maxlen, question_maxlen):\n",
    "    Xs, Xq, Y = [], [], []\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    \n",
    "    stories, questions, answers = data\n",
    "    for story, question, answer in zip(stories, questions, answers):\n",
    "        xs = [word2idx[w] for w in tokenize(flatten(story))]\n",
    "        xq = [word2idx[w] for w in tokenize(question)]\n",
    "        Xs.append(xs)\n",
    "        Xq.append(xq)\n",
    "        Y.append(word2idx[answer])\n",
    "\n",
    "    # 스토리와 질문은 각각의 최대 길이로 패딩\n",
    "    # 정답은 원-핫 인코딩\n",
    "    return pad_sequences(Xs, maxlen=story_maxlen),\\\n",
    "           pad_sequences(Xq, maxlen=question_maxlen),\\\n",
    "           to_categorical(Y, num_classes=len(word2idx) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "systematic-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xstrain, Xqtrain, Ytrain = vectorize(train_data, word2idx, story_max_len, question_max_len)\n",
    "Xstest, Xqtest, Ytest = vectorize(test_data, word2idx, story_max_len, question_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "liberal-identity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 40) (10000, 3) (10000, 25) (1000, 40) (1000, 3) (1000, 25)\n"
     ]
    }
   ],
   "source": [
    "print(Xstrain.shape, Xqtrain.shape, Ytrain.shape, Xstest.shape, Xqtest.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-hebrew",
   "metadata": {},
   "source": [
    "## 1번의 메모리 네트워크 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "three-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Permute, dot, add, concatenate\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Activation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "particular-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에포크 횟수\n",
    "train_epochs = 120\n",
    "# 배치 크기\n",
    "batch_size = 32\n",
    "# 임베딩 크기\n",
    "embed_size = 50\n",
    "# LSTM의 크기\n",
    "lstm_size = 64\n",
    "# 과적합 방지 기법인 드롭아웃 적용 비율\n",
    "dropout_rate = 0.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "engaged-glenn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stories : KerasTensor(type_spec=TensorSpec(shape=(None, 40), dtype=tf.float32, name='input_3'), name='input_3', description=\"created by layer 'input_3'\")\n",
      "Question: KerasTensor(type_spec=TensorSpec(shape=(None, 3), dtype=tf.float32, name='input_4'), name='input_4', description=\"created by layer 'input_4'\")\n"
     ]
    }
   ],
   "source": [
    "input_sequence = Input((story_max_len,))\n",
    "question = Input((question_max_len,))\n",
    " \n",
    "print('Stories :', input_sequence)\n",
    "print('Question:', question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "tribal-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스토리를 위한 첫 번째 임베딩. 그림에서의 Embedding A\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=embed_size))\n",
    "input_encoder_m.add(Dropout(dropout_rate))\n",
    "# 결과 : (samples, story_max_len, embed_size) / 샘플의 수, 문장의 최대 길이, 임베딩 벡터의 차원\n",
    " \n",
    "# 스토리를 위한 두 번째 임베딩. 그림에서의 Embedding C\n",
    "# 임베딩 벡터의 차원을 question_max_len(질문의 최대 길이)로 한다.\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=question_max_len))\n",
    "input_encoder_c.add(Dropout(dropout_rate))\n",
    "# 결과 : (samples, story_max_len, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이(임베딩 벡터의 차원)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "sticky-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문을 위한 임베딩. 그림에서의 Embedding B\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=embed_size,\n",
    "                               input_length=question_max_len))\n",
    "question_encoder.add(Dropout(dropout_rate))\n",
    "# 결과 : (samples, question_max_len, embed_size) / 샘플의 수, 질문의 최대 길이, 임베딩 벡터의 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "specified-request",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input encoded m KerasTensor(type_spec=TensorSpec(shape=(None, 40, 50), dtype=tf.float32, name=None), name='sequential_3/dropout_4/Identity:0', description=\"created by layer 'sequential_3'\") \n",
      "\n",
      "Input encoded c KerasTensor(type_spec=TensorSpec(shape=(None, 40, 3), dtype=tf.float32, name=None), name='sequential_4/dropout_5/Identity:0', description=\"created by layer 'sequential_4'\") \n",
      "\n",
      "Question encoded KerasTensor(type_spec=TensorSpec(shape=(None, 3, 50), dtype=tf.float32, name=None), name='sequential_5/dropout_6/Identity:0', description=\"created by layer 'sequential_5'\") \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 실질적인 임베딩 과정\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "\n",
    "print('Input encoded m', input_encoded_m, '\\n')\n",
    "print('Input encoded c', input_encoded_c, '\\n')\n",
    "print('Question encoded', question_encoded, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "light-windsor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match shape KerasTensor(type_spec=TensorSpec(shape=(None, 40, 3), dtype=tf.float32, name=None), name='activation_2/truediv:0', description=\"created by layer 'activation_2'\")\n"
     ]
    }
   ],
   "source": [
    "# 스토리 단어들과 질문 단어들 간의 유사도를 구하는 과정\n",
    "# 유사도는 내적을 사용한다.\n",
    "match = dot([input_encoded_m, question_encoded], axes=-1, normalize=False)\n",
    "match = Activation('softmax')(match)\n",
    "print('Match shape', match)\n",
    "# 결과 : (samples, story_max_len, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cooked-accessory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response shape KerasTensor(type_spec=TensorSpec(shape=(None, 3, 40), dtype=tf.float32, name=None), name='permute_1/transpose:0', description=\"created by layer 'permute_1'\")\n"
     ]
    }
   ],
   "source": [
    "# 매칭 유사도 행렬과 질문에 대한 임베딩을 더한다.\n",
    "response = add([match, input_encoded_c])  # (samples, story_maxlen, question_max_len)\n",
    "response = Permute((2, 1))(response)  # (samples, question_max_len, story_maxlen)\n",
    "print('Response shape', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "acceptable-protection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer shape KerasTensor(type_spec=TensorSpec(shape=(None, 3, 90), dtype=tf.float32, name=None), name='concatenate_1/concat:0', description=\"created by layer 'concatenate_1'\")\n"
     ]
    }
   ],
   "source": [
    "# concatenate the response vector with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])\n",
    "print('Answer shape', answer)\n",
    " \n",
    "answer = LSTM(lstm_size)(answer)  # Generate tensors of shape 32\n",
    "answer = Dropout(dropout_rate)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-selling",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "pressed-matter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "313/313 [==============================] - 3s 5ms/step - loss: 2.0449 - acc: 0.1699 - val_loss: 1.7897 - val_acc: 0.1670\n",
      "Epoch 2/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 1.7634 - acc: 0.2227 - val_loss: 1.6056 - val_acc: 0.3740\n",
      "Epoch 3/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.5777 - acc: 0.3765 - val_loss: 1.4875 - val_acc: 0.4200\n",
      "Epoch 4/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.5144 - acc: 0.3925 - val_loss: 1.4606 - val_acc: 0.4480\n",
      "Epoch 5/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.4690 - acc: 0.4318 - val_loss: 1.3894 - val_acc: 0.4760\n",
      "Epoch 6/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.4065 - acc: 0.4485 - val_loss: 1.3509 - val_acc: 0.4640\n",
      "Epoch 7/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3662 - acc: 0.4644 - val_loss: 1.3141 - val_acc: 0.4850\n",
      "Epoch 8/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3502 - acc: 0.4624 - val_loss: 1.3241 - val_acc: 0.4710\n",
      "Epoch 9/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3450 - acc: 0.4613 - val_loss: 1.3185 - val_acc: 0.4610\n",
      "Epoch 10/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3379 - acc: 0.4695 - val_loss: 1.3020 - val_acc: 0.4970\n",
      "Epoch 11/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3039 - acc: 0.4864 - val_loss: 1.2731 - val_acc: 0.5050\n",
      "Epoch 12/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3035 - acc: 0.4897 - val_loss: 1.2512 - val_acc: 0.5210\n",
      "Epoch 13/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2667 - acc: 0.5102 - val_loss: 1.2536 - val_acc: 0.5240\n",
      "Epoch 14/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2570 - acc: 0.5093 - val_loss: 1.2312 - val_acc: 0.5160\n",
      "Epoch 15/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2398 - acc: 0.5174 - val_loss: 1.2167 - val_acc: 0.5100\n",
      "Epoch 16/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2387 - acc: 0.5111 - val_loss: 1.2412 - val_acc: 0.5040\n",
      "Epoch 17/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2380 - acc: 0.5119 - val_loss: 1.2281 - val_acc: 0.5010\n",
      "Epoch 18/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2348 - acc: 0.5170 - val_loss: 1.1976 - val_acc: 0.5260\n",
      "Epoch 19/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2192 - acc: 0.5193 - val_loss: 1.2262 - val_acc: 0.5170\n",
      "Epoch 20/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.1929 - acc: 0.5265 - val_loss: 1.2010 - val_acc: 0.5230\n",
      "Epoch 21/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2093 - acc: 0.5150 - val_loss: 1.1579 - val_acc: 0.5330\n",
      "Epoch 22/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.1087 - acc: 0.5628 - val_loss: 0.9301 - val_acc: 0.6670\n",
      "Epoch 23/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.8676 - acc: 0.6772 - val_loss: 0.7198 - val_acc: 0.7370\n",
      "Epoch 24/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.7083 - acc: 0.7466 - val_loss: 0.6438 - val_acc: 0.7590\n",
      "Epoch 25/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6339 - acc: 0.7563 - val_loss: 0.5615 - val_acc: 0.7810\n",
      "Epoch 26/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5323 - acc: 0.7909 - val_loss: 0.4439 - val_acc: 0.8210\n",
      "Epoch 27/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4353 - acc: 0.8387 - val_loss: 0.3871 - val_acc: 0.8460\n",
      "Epoch 28/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4102 - acc: 0.8406 - val_loss: 0.3784 - val_acc: 0.8570\n",
      "Epoch 29/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3695 - acc: 0.8558 - val_loss: 0.3507 - val_acc: 0.8670\n",
      "Epoch 30/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3554 - acc: 0.8594 - val_loss: 0.3328 - val_acc: 0.8600\n",
      "Epoch 31/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3314 - acc: 0.8702 - val_loss: 0.3055 - val_acc: 0.8750\n",
      "Epoch 32/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2957 - acc: 0.8911 - val_loss: 0.2832 - val_acc: 0.8920\n",
      "Epoch 33/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2875 - acc: 0.8923 - val_loss: 0.2460 - val_acc: 0.9090\n",
      "Epoch 34/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2552 - acc: 0.9045 - val_loss: 0.2178 - val_acc: 0.9210\n",
      "Epoch 35/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2286 - acc: 0.9199 - val_loss: 0.1919 - val_acc: 0.9240\n",
      "Epoch 36/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2050 - acc: 0.9264 - val_loss: 0.1734 - val_acc: 0.9330\n",
      "Epoch 37/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.1903 - acc: 0.9276 - val_loss: 0.1562 - val_acc: 0.9410\n",
      "Epoch 38/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1789 - acc: 0.9368 - val_loss: 0.1496 - val_acc: 0.9390\n",
      "Epoch 39/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1681 - acc: 0.9417 - val_loss: 0.1363 - val_acc: 0.9550\n",
      "Epoch 40/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1586 - acc: 0.9451 - val_loss: 0.1215 - val_acc: 0.9530\n",
      "Epoch 41/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1475 - acc: 0.9508 - val_loss: 0.1100 - val_acc: 0.9510\n",
      "Epoch 42/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1337 - acc: 0.9532 - val_loss: 0.1036 - val_acc: 0.9700\n",
      "Epoch 43/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1260 - acc: 0.9572 - val_loss: 0.0980 - val_acc: 0.9610\n",
      "Epoch 44/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1118 - acc: 0.9607 - val_loss: 0.0714 - val_acc: 0.9740\n",
      "Epoch 45/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.1002 - acc: 0.9642 - val_loss: 0.0809 - val_acc: 0.9680\n",
      "Epoch 46/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0893 - acc: 0.9676 - val_loss: 0.0612 - val_acc: 0.9770\n",
      "Epoch 47/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0960 - acc: 0.9665 - val_loss: 0.0543 - val_acc: 0.9800\n",
      "Epoch 48/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0881 - acc: 0.9702 - val_loss: 0.0533 - val_acc: 0.9790\n",
      "Epoch 49/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0858 - acc: 0.9721 - val_loss: 0.0422 - val_acc: 0.9830\n",
      "Epoch 50/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0743 - acc: 0.9755 - val_loss: 0.0357 - val_acc: 0.9890\n",
      "Epoch 51/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0551 - acc: 0.9816 - val_loss: 0.0296 - val_acc: 0.9880\n",
      "Epoch 52/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0546 - acc: 0.9804 - val_loss: 0.0255 - val_acc: 0.9920\n",
      "Epoch 53/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0514 - acc: 0.9826 - val_loss: 0.0281 - val_acc: 0.9910\n",
      "Epoch 54/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0486 - acc: 0.9863 - val_loss: 0.0228 - val_acc: 0.9950\n",
      "Epoch 55/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0408 - acc: 0.9868 - val_loss: 0.0293 - val_acc: 0.9920\n",
      "Epoch 56/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0414 - acc: 0.9871 - val_loss: 0.0181 - val_acc: 0.9950\n",
      "Epoch 57/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0391 - acc: 0.9866 - val_loss: 0.0177 - val_acc: 0.9910\n",
      "Epoch 58/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0377 - acc: 0.9881 - val_loss: 0.0143 - val_acc: 0.9960\n",
      "Epoch 59/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0313 - acc: 0.9883 - val_loss: 0.0109 - val_acc: 0.9970\n",
      "Epoch 60/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0380 - acc: 0.9898 - val_loss: 0.0184 - val_acc: 0.9960\n",
      "Epoch 61/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0341 - acc: 0.9910 - val_loss: 0.0056 - val_acc: 1.0000\n",
      "Epoch 62/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0305 - acc: 0.9921 - val_loss: 0.0068 - val_acc: 0.9980\n",
      "Epoch 63/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0252 - acc: 0.9934 - val_loss: 0.0051 - val_acc: 0.9980\n",
      "Epoch 64/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0338 - acc: 0.9918 - val_loss: 0.0129 - val_acc: 0.9970\n",
      "Epoch 65/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0171 - acc: 0.9948 - val_loss: 0.0064 - val_acc: 0.9990\n",
      "Epoch 66/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0228 - acc: 0.9928 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 67/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0193 - acc: 0.9935 - val_loss: 0.0084 - val_acc: 0.9970\n",
      "Epoch 68/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0171 - acc: 0.9954 - val_loss: 0.0053 - val_acc: 0.9990\n",
      "Epoch 69/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0170 - acc: 0.9954 - val_loss: 0.0079 - val_acc: 0.9980\n",
      "Epoch 70/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0260 - acc: 0.9951 - val_loss: 0.0078 - val_acc: 0.9960\n",
      "Epoch 71/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0141 - acc: 0.9961 - val_loss: 0.0071 - val_acc: 0.9980\n",
      "Epoch 72/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0182 - acc: 0.9945 - val_loss: 0.0022 - val_acc: 0.9990\n",
      "Epoch 73/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0281 - acc: 0.9927 - val_loss: 9.9085e-04 - val_acc: 1.0000\n",
      "Epoch 74/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0238 - acc: 0.9933 - val_loss: 0.0023 - val_acc: 1.0000\n",
      "Epoch 75/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0206 - acc: 0.9949 - val_loss: 6.9850e-04 - val_acc: 1.0000\n",
      "Epoch 76/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0141 - acc: 0.9964 - val_loss: 0.0011 - val_acc: 1.0000\n",
      "Epoch 77/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0221 - acc: 0.9948 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 78/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0287 - acc: 0.9932 - val_loss: 3.8368e-04 - val_acc: 1.0000\n",
      "Epoch 79/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0268 - acc: 0.9946 - val_loss: 6.4196e-04 - val_acc: 1.0000\n",
      "Epoch 80/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0124 - acc: 0.9969 - val_loss: 8.5067e-04 - val_acc: 1.0000\n",
      "Epoch 81/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0136 - acc: 0.9944 - val_loss: 0.0074 - val_acc: 0.9980\n",
      "Epoch 82/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0183 - acc: 0.9954 - val_loss: 5.8746e-04 - val_acc: 1.0000\n",
      "Epoch 83/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0150 - acc: 0.9963 - val_loss: 5.4470e-04 - val_acc: 1.0000\n",
      "Epoch 84/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0127 - acc: 0.9974 - val_loss: 8.9373e-04 - val_acc: 1.0000\n",
      "Epoch 85/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0116 - acc: 0.9963 - val_loss: 4.4212e-04 - val_acc: 1.0000\n",
      "Epoch 86/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0107 - acc: 0.9982 - val_loss: 3.3715e-04 - val_acc: 1.0000\n",
      "Epoch 87/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0138 - acc: 0.9965 - val_loss: 0.0047 - val_acc: 0.9980\n",
      "Epoch 88/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0131 - acc: 0.9960 - val_loss: 3.5258e-04 - val_acc: 1.0000\n",
      "Epoch 89/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0165 - acc: 0.9960 - val_loss: 5.2561e-04 - val_acc: 1.0000\n",
      "Epoch 90/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0099 - acc: 0.9972 - val_loss: 6.7341e-04 - val_acc: 1.0000\n",
      "Epoch 91/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0128 - acc: 0.9962 - val_loss: 4.5756e-04 - val_acc: 1.0000\n",
      "Epoch 92/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0218 - acc: 0.9935 - val_loss: 0.0033 - val_acc: 0.9990\n",
      "Epoch 93/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0332 - acc: 0.9947 - val_loss: 1.7514e-04 - val_acc: 1.0000\n",
      "Epoch 94/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0113 - acc: 0.9960 - val_loss: 2.0058e-04 - val_acc: 1.0000\n",
      "Epoch 95/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0130 - acc: 0.9959 - val_loss: 1.1494e-04 - val_acc: 1.0000\n",
      "Epoch 96/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0161 - acc: 0.9963 - val_loss: 0.0045 - val_acc: 0.9980\n",
      "Epoch 97/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0165 - acc: 0.9959 - val_loss: 6.0486e-04 - val_acc: 1.0000\n",
      "Epoch 98/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0111 - acc: 0.9978 - val_loss: 3.5824e-04 - val_acc: 1.0000\n",
      "Epoch 99/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0118 - acc: 0.9960 - val_loss: 0.0029 - val_acc: 0.9980\n",
      "Epoch 100/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0135 - acc: 0.9965 - val_loss: 1.5481e-04 - val_acc: 1.0000\n",
      "Epoch 101/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0130 - acc: 0.9966 - val_loss: 1.4247e-04 - val_acc: 1.0000\n",
      "Epoch 102/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0143 - acc: 0.9966 - val_loss: 9.9917e-05 - val_acc: 1.0000\n",
      "Epoch 103/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0226 - acc: 0.9960 - val_loss: 5.4545e-05 - val_acc: 1.0000\n",
      "Epoch 104/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0124 - acc: 0.9969 - val_loss: 4.0965e-05 - val_acc: 1.0000\n",
      "Epoch 105/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0204 - acc: 0.9953 - val_loss: 2.7096e-04 - val_acc: 1.0000\n",
      "Epoch 106/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0234 - acc: 0.9963 - val_loss: 4.9833e-05 - val_acc: 1.0000\n",
      "Epoch 107/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0135 - acc: 0.9974 - val_loss: 6.3722e-05 - val_acc: 1.0000\n",
      "Epoch 108/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0161 - acc: 0.9965 - val_loss: 6.7336e-04 - val_acc: 1.0000\n",
      "Epoch 109/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0126 - acc: 0.9962 - val_loss: 5.4799e-05 - val_acc: 1.0000\n",
      "Epoch 110/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0166 - acc: 0.9966 - val_loss: 6.3938e-05 - val_acc: 1.0000\n",
      "Epoch 111/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0194 - acc: 0.9975 - val_loss: 0.0025 - val_acc: 0.9990\n",
      "Epoch 112/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0242 - acc: 0.9950 - val_loss: 1.2436e-04 - val_acc: 1.0000\n",
      "Epoch 113/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0156 - acc: 0.9967 - val_loss: 3.5797e-04 - val_acc: 1.0000\n",
      "Epoch 114/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0159 - acc: 0.9947 - val_loss: 1.0980e-04 - val_acc: 1.0000\n",
      "Epoch 115/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0193 - acc: 0.9962 - val_loss: 4.2399e-05 - val_acc: 1.0000\n",
      "Epoch 116/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0072 - acc: 0.9979 - val_loss: 1.9742e-05 - val_acc: 1.0000\n",
      "Epoch 117/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0113 - acc: 0.9964 - val_loss: 1.0130e-04 - val_acc: 1.0000\n",
      "Epoch 118/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0148 - acc: 0.9972 - val_loss: 5.4391e-05 - val_acc: 1.0000\n",
      "Epoch 119/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0118 - acc: 0.9954 - val_loss: 1.2111e-04 - val_acc: 1.0000\n",
      "Epoch 120/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0147 - acc: 0.9966 - val_loss: 3.8931e-04 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 모델 컴파일\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    " \n",
    "# 테스트 데이터를 검증 데이터로 사용하면서 모델 훈련 시작\n",
    "history = model.fit([Xstrain, Xqtrain],\n",
    "         Ytrain, batch_size, train_epochs,\n",
    "         validation_data=([Xstest, Xqtest], Ytest))\n",
    " \n",
    "# 훈련 후에는 모델 저장\n",
    "model_path = os.getenv('HOME')+'/aiffel/babi_memory_net/model.h5'\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "visible-pride",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 3.8931e-04 - acc: 1.0000\n",
      "\n",
      " 테스트 정확도: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate([Xstest, Xqtest], Ytest)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "rough-indianapolis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot accuracy and loss plot\n",
    "plt.subplot(211)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(history.history[\"acc\"], color=\"g\", label=\"train\")\n",
    "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(history.history[\"loss\"], color=\"g\", label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# labels\n",
    "ytest = np.argmax(Ytest, axis=1)\n",
    "\n",
    "# get predictions\n",
    "Ytest_ = model.predict([Xstest, Xqtest])\n",
    "ytest_ = np.argmax(Ytest_, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "smart-reach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문                  |실제값    |예측값\n",
      "---------------------------------------\n",
      "은경이는 어디야 ?          : 복도       복도\n",
      "필웅이는 어디야 ?          : 화장실      화장실\n",
      "경임이는 어디야 ?          : 부엌       부엌\n",
      "경임이는 어디야 ?          : 복도       복도\n",
      "경임이는 어디야 ?          : 부엌       부엌\n",
      "경임이는 어디야 ?          : 복도       복도\n",
      "경임이는 어디야 ?          : 정원       정원\n",
      "수종이는 어디야 ?          : 복도       복도\n",
      "경임이는 어디야 ?          : 사무실      사무실\n",
      "수종이는 어디야 ?          : 사무실      사무실\n",
      "필웅이는 어디야 ?          : 부엌       부엌\n",
      "필웅이는 어디야 ?          : 정원       정원\n",
      "수종이는 어디야 ?          : 사무실      사무실\n",
      "필웅이는 어디야 ?          : 침실       침실\n",
      "필웅이는 어디야 ?          : 침실       침실\n",
      "은경이는 어디야 ?          : 부엌       부엌\n",
      "은경이는 어디야 ?          : 정원       정원\n",
      "은경이는 어디야 ?          : 부엌       부엌\n",
      "수종이는 어디야 ?          : 사무실      사무실\n",
      "은경이는 어디야 ?          : 부엌       부엌\n",
      "필웅이는 어디야 ?          : 복도       복도\n",
      "은경이는 어디야 ?          : 사무실      사무실\n",
      "은경이는 어디야 ?          : 사무실      사무실\n",
      "경임이는 어디야 ?          : 복도       복도\n",
      "수종이는 어디야 ?          : 침실       침실\n",
      "경임이는 어디야 ?          : 침실       침실\n",
      "필웅이는 어디야 ?          : 침실       침실\n",
      "수종이는 어디야 ?          : 부엌       부엌\n",
      "수종이는 어디야 ?          : 부엌       부엌\n",
      "수종이는 어디야 ?          : 부엌       부엌\n"
     ]
    }
   ],
   "source": [
    "NUM_DISPLAY = 30\n",
    "\n",
    "print(\"{:20}|{:7}|{}\".format(\"질문\", \"실제값\", \"예측값\"))\n",
    "print(39 * \"-\")\n",
    "\n",
    "for i in range(NUM_DISPLAY):\n",
    "    question = \" \".join([idx2word[x] for x in Xqtest[i].tolist()])\n",
    "    label = idx2word[ytest[i]]\n",
    "    prediction = idx2word[ytest_[i]]\n",
    "    print(\"{:20}: {:8} {}\".format(question, label, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-customer",
   "metadata": {},
   "source": [
    "## 2. 불용어를 제거한 데이터를 사용한 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-finnish",
   "metadata": {},
   "source": [
    "## 불용어를 제거하고 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "angry-makeup",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "wireless-combat",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.add_dictionary('은경이', 'Noun')\n",
    "twitter.add_dictionary('경임이', 'Noun')\n",
    "twitter.add_dictionary('수종이', 'Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aerial-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다'] \n",
    "def tokenize_stop(sent):\n",
    "    return twitter.morphs(sent) # 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "intermediate-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_stop(train_data, test_data):\n",
    "    counter = FreqDist()\n",
    "    \n",
    "    # 두 문장의 story를 하나의 문장으로 통합하는 함수\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "\n",
    "    # 각 샘플의 길이를 저장하는 리스트\n",
    "    story_len = []\n",
    "    question_len = []\n",
    "    stopwords = ['의','가','이','야','은','들','는','좀','잘','걍','과','도','를','으로','로','자','에','와','한','하다'] \n",
    "    for stories, questions, answers in [train_data, test_data]:\n",
    "        for story in stories:\n",
    "            stories = tokenize_stop(flatten(story)) # 스토리의 문장들을 펼친 후 토큰화\n",
    "            stories = [word for word in stories if not word in stopwords]\n",
    "            story_len.append(len(stories)) # 각 story의 길이 저장\n",
    "            for word in stories: # 단어 집합에 단어 추가\n",
    "                counter[word] += 1\n",
    "        for question in questions:\n",
    "            question = tokenize_stop(question)\n",
    "            question = [word for word in question if not word in stopwords]\n",
    "            question_len.append(len(question))\n",
    "            for word in question:\n",
    "                counter[word] += 1\n",
    "        for answer in answers:\n",
    "            answer = tokenize_stop(answer)\n",
    "            answer = [word for word in answer if not word in stopwords]\n",
    "            for word in answer:\n",
    "                counter[word] += 1\n",
    "\n",
    "    # 단어장 생성\n",
    "    word2idx = {word : (idx + 1) for idx, (word, _) in enumerate(counter.most_common())}\n",
    "    idx2word = {idx : word for word, idx in word2idx.items()}\n",
    "\n",
    "    # 가장 긴 샘플의 길이\n",
    "    story_max_len = np.max(story_len)\n",
    "    question_max_len = np.max(question_len)\n",
    "\n",
    "    return word2idx, idx2word, story_max_len, question_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "demographic-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx_stop, idx2word_stop, story_max_len_stop, question_max_len_stop = preprocess_data_stop(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "decimal-preparation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 1, '했습니다': 2, '경임이': 3, '은경이': 4, '수종이': 5, '필웅이': 6, '이동': 7, '가버렸습니다': 8, '뛰어갔습니다': 9, '복귀': 10, '화장실': 11, '정원': 12, '복도': 13, '갔습니다': 14, '사무실': 15, '부엌': 16, '침실': 17, '어디': 18, '?': 19}\n"
     ]
    }
   ],
   "source": [
    "print(word2idx_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "limited-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx_stop) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "arbitrary-proceeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스토리의 최대 길이 : 50\n",
      "질문의 최대 길이 : 3\n"
     ]
    }
   ],
   "source": [
    "print('스토리의 최대 길이 :',story_max_len_stop)\n",
    "print('질문의 최대 길이 :',question_max_len_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "engaging-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_stop(data, word2idx, story_maxlen, question_maxlen):\n",
    "    Xs, Xq, Y = [], [], []\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "     \n",
    "    stopwords = ['의','가','이','야','은','들','는','좀','잘','걍','과','도','를','으로','로','자','에','와','한','하다']\n",
    "    \n",
    "    stories, questions, answers = data\n",
    "    \n",
    "    for story, question, answer in zip(stories, questions, answers):\n",
    "        xs = [word2idx[w] for w in tokenize_stop(flatten(story))]\n",
    "        xs = [word for word in xs if not word in stopwords]\n",
    "        \n",
    "        xq = [word2idx[w] for w in tokenize_stop(question)]\n",
    "        xq = [word for word in xq if not word in stopwords]\n",
    "        \n",
    "        Xs.append(xs)\n",
    "        Xq.append(xq)\n",
    "        Y.append(word2idx[answer])\n",
    "\n",
    "    # 스토리와 질문은 각각의 최대 길이로 패딩\n",
    "    # 정답은 원-핫 인코딩\n",
    "    return pad_sequences(Xs, maxlen=story_maxlen),\\\n",
    "           pad_sequences(Xq, maxlen=question_maxlen),\\\n",
    "           to_categorical(Y, num_classes=len(word2idx) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fifth-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_stop(data, word2idx, story_maxlen, question_maxlen):\n",
    "    Xs, Xq, Y = [], [], []\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "     \n",
    "    stopwords = ['의','가','이','야','은','들','는','좀','잘','걍','과','도','를','으로','로','자','에','와','한','하다']\n",
    "    \n",
    "    stories, questions, answers = data\n",
    "    \n",
    "    for story, question, answer in zip(stories, questions, answers):\n",
    "        xs = [w for w in tokenize_stop(flatten(story))]\n",
    "        xs = [w for w in xs if not w in stopwords]\n",
    "        xs = [word2idx[w] for w in xs]\n",
    "        \n",
    "        xq = [w for w in tokenize_stop(question)]\n",
    "        xq = [w for w in xq if not w in stopwords]\n",
    "        xq = [word2idx[w] for w in xq]\n",
    "        \n",
    "        Xs.append(xs)\n",
    "        Xq.append(xq)\n",
    "        Y.append(word2idx[answer])\n",
    "\n",
    "    # 스토리와 질문은 각각의 최대 길이로 패딩\n",
    "    # 정답은 원-핫 인코딩\n",
    "    return pad_sequences(Xs, maxlen=story_maxlen),\\\n",
    "           pad_sequences(Xq, maxlen=question_maxlen),\\\n",
    "           to_categorical(Y, num_classes=len(word2idx) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "leading-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xstrain_stop, Xqtrain_stop, Ytrain_stop = vectorize_stop(train_data, word2idx_stop, story_max_len_stop, question_max_len_stop)\n",
    "Xstest_stop, Xqtest_stop, Ytest_stop = vectorize_stop(test_data, word2idx_stop, story_max_len_stop, question_max_len_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "agricultural-exception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 50) (10000, 3) (10000, 20) (1000, 50) (1000, 3) (1000, 20)\n"
     ]
    }
   ],
   "source": [
    "print(Xstrain_stop.shape, Xqtrain_stop.shape, Ytrain_stop.shape, Xstest_stop.shape, Xqtest_stop.shape, Ytest_stop.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-walnut",
   "metadata": {},
   "source": [
    "## 2번의 메모리 네트워크 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "worse-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Permute, dot, add, concatenate\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Activation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "advised-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에포크 횟수\n",
    "train_epochs = 120\n",
    "# 배치 크기\n",
    "batch_size = 32\n",
    "# 임베딩 크기\n",
    "embed_size = 50\n",
    "# LSTM의 크기\n",
    "lstm_size = 64\n",
    "# 과적합 방지 기법인 드롭아웃 적용 비율\n",
    "dropout_rate = 0.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "assigned-attempt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stories : KerasTensor(type_spec=TensorSpec(shape=(None, 50), dtype=tf.float32, name='input_5'), name='input_5', description=\"created by layer 'input_5'\")\n",
      "Question: KerasTensor(type_spec=TensorSpec(shape=(None, 3), dtype=tf.float32, name='input_6'), name='input_6', description=\"created by layer 'input_6'\")\n"
     ]
    }
   ],
   "source": [
    "input_sequence = Input((story_max_len_stop,))\n",
    "question = Input((question_max_len_stop,))\n",
    " \n",
    "print('Stories :', input_sequence)\n",
    "print('Question:', question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "interior-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스토리를 위한 첫 번째 임베딩. 그림에서의 Embedding A\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=embed_size))\n",
    "input_encoder_m.add(Dropout(dropout_rate))\n",
    "# 결과 : (samples, story_max_len, embed_size) / 샘플의 수, 문장의 최대 길이, 임베딩 벡터의 차원\n",
    " \n",
    "# 스토리를 위한 두 번째 임베딩. 그림에서의 Embedding C\n",
    "# 임베딩 벡터의 차원을 question_max_len(질문의 최대 길이)로 한다.\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=question_max_len))\n",
    "input_encoder_c.add(Dropout(dropout_rate))\n",
    "# 결과 : (samples, story_max_len, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이(임베딩 벡터의 차원)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "apparent-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문을 위한 임베딩. 그림에서의 Embedding B\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=embed_size,\n",
    "                               input_length=question_max_len))\n",
    "question_encoder.add(Dropout(dropout_rate))\n",
    "# 결과 : (samples, question_max_len, embed_size) / 샘플의 수, 질문의 최대 길이, 임베딩 벡터의 차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "different-furniture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input encoded m KerasTensor(type_spec=TensorSpec(shape=(None, 50, 50), dtype=tf.float32, name=None), name='sequential_6/dropout_8/Identity:0', description=\"created by layer 'sequential_6'\") \n",
      "\n",
      "Input encoded c KerasTensor(type_spec=TensorSpec(shape=(None, 50, 3), dtype=tf.float32, name=None), name='sequential_7/dropout_9/Identity:0', description=\"created by layer 'sequential_7'\") \n",
      "\n",
      "Question encoded KerasTensor(type_spec=TensorSpec(shape=(None, 3, 50), dtype=tf.float32, name=None), name='sequential_8/dropout_10/Identity:0', description=\"created by layer 'sequential_8'\") \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 실질적인 임베딩 과정\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "\n",
    "print('Input encoded m', input_encoded_m, '\\n')\n",
    "print('Input encoded c', input_encoded_c, '\\n')\n",
    "print('Question encoded', question_encoded, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "collective-davis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match shape KerasTensor(type_spec=TensorSpec(shape=(None, 50, 3), dtype=tf.float32, name=None), name='activation_4/truediv:0', description=\"created by layer 'activation_4'\")\n"
     ]
    }
   ],
   "source": [
    "# 스토리 단어들과 질문 단어들 간의 유사도를 구하는 과정\n",
    "# 유사도는 내적을 사용한다.\n",
    "match = dot([input_encoded_m, question_encoded], axes=-1, normalize=False)\n",
    "match = Activation('softmax')(match)\n",
    "print('Match shape', match)\n",
    "# 결과 : (samples, story_max_len, question_max_len) / 샘플의 수, 문장의 최대 길이, 질문의 최대 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "historic-password",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response shape KerasTensor(type_spec=TensorSpec(shape=(None, 3, 50), dtype=tf.float32, name=None), name='permute_2/transpose:0', description=\"created by layer 'permute_2'\")\n"
     ]
    }
   ],
   "source": [
    "# 매칭 유사도 행렬과 질문에 대한 임베딩을 더한다.\n",
    "response = add([match, input_encoded_c])  # (samples, story_maxlen, question_max_len)\n",
    "response = Permute((2, 1))(response)  # (samples, question_max_len, story_maxlen)\n",
    "print('Response shape', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "verbal-topic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer shape KerasTensor(type_spec=TensorSpec(shape=(None, 3, 100), dtype=tf.float32, name=None), name='concatenate_2/concat:0', description=\"created by layer 'concatenate_2'\")\n"
     ]
    }
   ],
   "source": [
    "# concatenate the response vector with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])\n",
    "print('Answer shape', answer)\n",
    " \n",
    "answer = LSTM(lstm_size)(answer)  # Generate tensors of shape 32\n",
    "answer = Dropout(dropout_rate)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-daily",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "joined-current",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "313/313 [==============================] - 4s 5ms/step - loss: 2.0161 - acc: 0.1569 - val_loss: 1.7855 - val_acc: 0.1860\n",
      "Epoch 2/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.7722 - acc: 0.2054 - val_loss: 1.6673 - val_acc: 0.2570\n",
      "Epoch 3/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.6447 - acc: 0.2845 - val_loss: 1.5573 - val_acc: 0.3710\n",
      "Epoch 4/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.5440 - acc: 0.3660 - val_loss: 1.5359 - val_acc: 0.3620\n",
      "Epoch 5/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.5061 - acc: 0.3943 - val_loss: 1.5302 - val_acc: 0.3800\n",
      "Epoch 6/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.4661 - acc: 0.4262 - val_loss: 1.3830 - val_acc: 0.4790\n",
      "Epoch 7/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 1.4089 - acc: 0.4587 - val_loss: 1.3573 - val_acc: 0.4890\n",
      "Epoch 8/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 1.3751 - acc: 0.4701 - val_loss: 1.3521 - val_acc: 0.4870\n",
      "Epoch 9/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3659 - acc: 0.4699 - val_loss: 1.3850 - val_acc: 0.4700\n",
      "Epoch 10/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3644 - acc: 0.4640 - val_loss: 1.3313 - val_acc: 0.4990\n",
      "Epoch 11/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3446 - acc: 0.4788 - val_loss: 1.3434 - val_acc: 0.4850\n",
      "Epoch 12/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3398 - acc: 0.4755 - val_loss: 1.3329 - val_acc: 0.5030\n",
      "Epoch 13/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3296 - acc: 0.4996 - val_loss: 1.3354 - val_acc: 0.4750\n",
      "Epoch 14/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3309 - acc: 0.4845 - val_loss: 1.3378 - val_acc: 0.4820\n",
      "Epoch 15/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3218 - acc: 0.4813 - val_loss: 1.3231 - val_acc: 0.5050\n",
      "Epoch 16/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3094 - acc: 0.5005 - val_loss: 1.3545 - val_acc: 0.4630\n",
      "Epoch 17/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3117 - acc: 0.4890 - val_loss: 1.2798 - val_acc: 0.5210\n",
      "Epoch 18/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2921 - acc: 0.5026 - val_loss: 1.2699 - val_acc: 0.5290\n",
      "Epoch 19/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2923 - acc: 0.4980 - val_loss: 1.2927 - val_acc: 0.5230\n",
      "Epoch 20/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2806 - acc: 0.5063 - val_loss: 1.2710 - val_acc: 0.5120\n",
      "Epoch 21/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2621 - acc: 0.5156 - val_loss: 1.2679 - val_acc: 0.5320\n",
      "Epoch 22/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2493 - acc: 0.5229 - val_loss: 1.2988 - val_acc: 0.5070\n",
      "Epoch 23/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2268 - acc: 0.5232 - val_loss: 1.2516 - val_acc: 0.5230\n",
      "Epoch 24/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2232 - acc: 0.5308 - val_loss: 1.2500 - val_acc: 0.5370\n",
      "Epoch 25/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2216 - acc: 0.5294 - val_loss: 1.2568 - val_acc: 0.5270\n",
      "Epoch 26/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2073 - acc: 0.5364 - val_loss: 1.2489 - val_acc: 0.5390\n",
      "Epoch 27/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.1935 - acc: 0.5449 - val_loss: 1.1906 - val_acc: 0.5770\n",
      "Epoch 28/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.1411 - acc: 0.5756 - val_loss: 1.1221 - val_acc: 0.6080\n",
      "Epoch 29/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.0610 - acc: 0.6207 - val_loss: 0.9738 - val_acc: 0.6740\n",
      "Epoch 30/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.9363 - acc: 0.6670 - val_loss: 0.8713 - val_acc: 0.7120\n",
      "Epoch 31/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.8448 - acc: 0.7059 - val_loss: 0.8270 - val_acc: 0.7170\n",
      "Epoch 32/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.7838 - acc: 0.7222 - val_loss: 0.7985 - val_acc: 0.7380\n",
      "Epoch 33/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.7509 - acc: 0.7312 - val_loss: 0.7274 - val_acc: 0.7530\n",
      "Epoch 34/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6916 - acc: 0.7540 - val_loss: 0.6570 - val_acc: 0.7610\n",
      "Epoch 35/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6578 - acc: 0.7629 - val_loss: 0.6174 - val_acc: 0.7660\n",
      "Epoch 36/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5939 - acc: 0.7861 - val_loss: 0.5632 - val_acc: 0.7820\n",
      "Epoch 37/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5821 - acc: 0.7842 - val_loss: 0.5274 - val_acc: 0.8080\n",
      "Epoch 38/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5331 - acc: 0.8064 - val_loss: 0.5037 - val_acc: 0.8210\n",
      "Epoch 39/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5023 - acc: 0.8115 - val_loss: 0.5078 - val_acc: 0.8220\n",
      "Epoch 40/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4944 - acc: 0.8171 - val_loss: 0.4700 - val_acc: 0.8260\n",
      "Epoch 41/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4617 - acc: 0.8330 - val_loss: 0.4602 - val_acc: 0.8160\n",
      "Epoch 42/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4623 - acc: 0.8328 - val_loss: 0.4253 - val_acc: 0.8420\n",
      "Epoch 43/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4432 - acc: 0.8348 - val_loss: 0.4215 - val_acc: 0.8530\n",
      "Epoch 44/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4245 - acc: 0.8440 - val_loss: 0.4178 - val_acc: 0.8460\n",
      "Epoch 45/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4109 - acc: 0.8520 - val_loss: 0.4041 - val_acc: 0.8440\n",
      "Epoch 46/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3937 - acc: 0.8561 - val_loss: 0.4004 - val_acc: 0.8570\n",
      "Epoch 47/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3884 - acc: 0.8597 - val_loss: 0.3904 - val_acc: 0.8670\n",
      "Epoch 48/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3760 - acc: 0.8592 - val_loss: 0.3739 - val_acc: 0.8650\n",
      "Epoch 49/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3748 - acc: 0.8595 - val_loss: 0.3855 - val_acc: 0.8640\n",
      "Epoch 50/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3511 - acc: 0.8742 - val_loss: 0.3342 - val_acc: 0.8860\n",
      "Epoch 51/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3312 - acc: 0.8846 - val_loss: 0.3401 - val_acc: 0.8790\n",
      "Epoch 52/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3247 - acc: 0.8831 - val_loss: 0.3217 - val_acc: 0.8810\n",
      "Epoch 53/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3192 - acc: 0.8814 - val_loss: 0.3434 - val_acc: 0.8680\n",
      "Epoch 54/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3083 - acc: 0.8868 - val_loss: 0.3026 - val_acc: 0.9010\n",
      "Epoch 55/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2990 - acc: 0.8847 - val_loss: 0.3065 - val_acc: 0.8980\n",
      "Epoch 56/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2974 - acc: 0.8915 - val_loss: 0.2917 - val_acc: 0.8940\n",
      "Epoch 57/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2915 - acc: 0.8934 - val_loss: 0.2869 - val_acc: 0.9040\n",
      "Epoch 58/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2821 - acc: 0.8982 - val_loss: 0.2942 - val_acc: 0.8970\n",
      "Epoch 59/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2667 - acc: 0.9036 - val_loss: 0.3211 - val_acc: 0.8930\n",
      "Epoch 60/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2656 - acc: 0.9106 - val_loss: 0.2958 - val_acc: 0.9000\n",
      "Epoch 61/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2625 - acc: 0.9085 - val_loss: 0.2789 - val_acc: 0.9030\n",
      "Epoch 62/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2457 - acc: 0.9131 - val_loss: 0.2765 - val_acc: 0.9080\n",
      "Epoch 63/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2470 - acc: 0.9120 - val_loss: 0.2616 - val_acc: 0.9050\n",
      "Epoch 64/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2412 - acc: 0.9171 - val_loss: 0.2938 - val_acc: 0.8990\n",
      "Epoch 65/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2423 - acc: 0.9151 - val_loss: 0.2581 - val_acc: 0.9160\n",
      "Epoch 66/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2268 - acc: 0.9175 - val_loss: 0.2647 - val_acc: 0.9130\n",
      "Epoch 67/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2366 - acc: 0.9148 - val_loss: 0.2506 - val_acc: 0.9070\n",
      "Epoch 68/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2320 - acc: 0.9176 - val_loss: 0.2577 - val_acc: 0.9080\n",
      "Epoch 69/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2164 - acc: 0.9230 - val_loss: 0.2545 - val_acc: 0.9060\n",
      "Epoch 70/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2076 - acc: 0.9264 - val_loss: 0.2452 - val_acc: 0.9110\n",
      "Epoch 71/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2004 - acc: 0.9289 - val_loss: 0.2543 - val_acc: 0.9140\n",
      "Epoch 72/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1930 - acc: 0.9322 - val_loss: 0.2488 - val_acc: 0.9120\n",
      "Epoch 73/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1936 - acc: 0.9315 - val_loss: 0.2279 - val_acc: 0.9210\n",
      "Epoch 74/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1897 - acc: 0.9310 - val_loss: 0.2287 - val_acc: 0.9190\n",
      "Epoch 75/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1855 - acc: 0.9346 - val_loss: 0.2258 - val_acc: 0.9280\n",
      "Epoch 76/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1817 - acc: 0.9368 - val_loss: 0.2751 - val_acc: 0.9200\n",
      "Epoch 77/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1801 - acc: 0.9366 - val_loss: 0.2171 - val_acc: 0.9310\n",
      "Epoch 78/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1838 - acc: 0.9336 - val_loss: 0.2095 - val_acc: 0.9320\n",
      "Epoch 79/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1701 - acc: 0.9439 - val_loss: 0.2217 - val_acc: 0.9260\n",
      "Epoch 80/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1639 - acc: 0.9417 - val_loss: 0.2416 - val_acc: 0.9190\n",
      "Epoch 81/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1632 - acc: 0.9429 - val_loss: 0.2052 - val_acc: 0.9320\n",
      "Epoch 82/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1655 - acc: 0.9395 - val_loss: 0.2237 - val_acc: 0.9320\n",
      "Epoch 83/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1560 - acc: 0.9472 - val_loss: 0.1868 - val_acc: 0.9390\n",
      "Epoch 84/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1506 - acc: 0.9459 - val_loss: 0.2194 - val_acc: 0.9290\n",
      "Epoch 85/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1475 - acc: 0.9471 - val_loss: 0.2239 - val_acc: 0.9230\n",
      "Epoch 86/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.1581 - acc: 0.9465 - val_loss: 0.1985 - val_acc: 0.9370\n",
      "Epoch 87/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.1496 - acc: 0.9493 - val_loss: 0.2171 - val_acc: 0.9320\n",
      "Epoch 88/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.1423 - acc: 0.9558 - val_loss: 0.2089 - val_acc: 0.9380\n",
      "Epoch 89/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1424 - acc: 0.9528 - val_loss: 0.1821 - val_acc: 0.9340\n",
      "Epoch 90/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1389 - acc: 0.9487 - val_loss: 0.1986 - val_acc: 0.9300\n",
      "Epoch 91/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1428 - acc: 0.9518 - val_loss: 0.1839 - val_acc: 0.9430\n",
      "Epoch 92/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1424 - acc: 0.9525 - val_loss: 0.2123 - val_acc: 0.9290\n",
      "Epoch 93/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.1256 - acc: 0.9562 - val_loss: 0.1758 - val_acc: 0.9450\n",
      "Epoch 94/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1406 - acc: 0.9537 - val_loss: 0.1895 - val_acc: 0.9420\n",
      "Epoch 95/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1269 - acc: 0.9586 - val_loss: 0.1783 - val_acc: 0.9430\n",
      "Epoch 96/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1090 - acc: 0.9628 - val_loss: 0.2142 - val_acc: 0.9250\n",
      "Epoch 97/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.1286 - acc: 0.9559 - val_loss: 0.1811 - val_acc: 0.9390\n",
      "Epoch 98/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1258 - acc: 0.9550 - val_loss: 0.1808 - val_acc: 0.9450\n",
      "Epoch 99/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1181 - acc: 0.9590 - val_loss: 0.1888 - val_acc: 0.9430\n",
      "Epoch 100/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1281 - acc: 0.9561 - val_loss: 0.1965 - val_acc: 0.9390\n",
      "Epoch 101/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1097 - acc: 0.9628 - val_loss: 0.1852 - val_acc: 0.9350\n",
      "Epoch 102/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1090 - acc: 0.9670 - val_loss: 0.1796 - val_acc: 0.9390\n",
      "Epoch 103/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1161 - acc: 0.9605 - val_loss: 0.2073 - val_acc: 0.9340\n",
      "Epoch 104/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.1025 - acc: 0.9689 - val_loss: 0.1733 - val_acc: 0.9390\n",
      "Epoch 105/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1061 - acc: 0.9636 - val_loss: 0.1861 - val_acc: 0.9390\n",
      "Epoch 106/120\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.1158 - acc: 0.9616 - val_loss: 0.2160 - val_acc: 0.9260\n",
      "Epoch 107/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1059 - acc: 0.9633 - val_loss: 0.1577 - val_acc: 0.9500\n",
      "Epoch 108/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1054 - acc: 0.9613 - val_loss: 0.1868 - val_acc: 0.9430\n",
      "Epoch 109/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1043 - acc: 0.9660 - val_loss: 0.1587 - val_acc: 0.9460\n",
      "Epoch 110/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1001 - acc: 0.9682 - val_loss: 0.1626 - val_acc: 0.9510\n",
      "Epoch 111/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0907 - acc: 0.9688 - val_loss: 0.1714 - val_acc: 0.9510\n",
      "Epoch 112/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0980 - acc: 0.9685 - val_loss: 0.1728 - val_acc: 0.9430\n",
      "Epoch 113/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0821 - acc: 0.9724 - val_loss: 0.1709 - val_acc: 0.9410\n",
      "Epoch 114/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0922 - acc: 0.9703 - val_loss: 0.1512 - val_acc: 0.9520\n",
      "Epoch 115/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1012 - acc: 0.9692 - val_loss: 0.1734 - val_acc: 0.9540\n",
      "Epoch 116/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0902 - acc: 0.9729 - val_loss: 0.1659 - val_acc: 0.9530\n",
      "Epoch 117/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0938 - acc: 0.9680 - val_loss: 0.1827 - val_acc: 0.9480\n",
      "Epoch 118/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0795 - acc: 0.9729 - val_loss: 0.1624 - val_acc: 0.9530\n",
      "Epoch 119/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1049 - acc: 0.9706 - val_loss: 0.1482 - val_acc: 0.9520\n",
      "Epoch 120/120\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0744 - acc: 0.9782 - val_loss: 0.1575 - val_acc: 0.9520\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 모델 컴파일\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    " \n",
    "# 테스트 데이터를 검증 데이터로 사용하면서 모델 훈련 시작\n",
    "history = model.fit([Xstrain_stop, Xqtrain_stop],\n",
    "         Ytrain_stop, batch_size, train_epochs,\n",
    "         validation_data=([Xstest_stop, Xqtest_stop], Ytest_stop))\n",
    " \n",
    "# 훈련 후에는 모델 저장\n",
    "model_path = os.getenv('HOME')+'/aiffel/babi_memory_net/model.h5'\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "directed-participant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1575 - acc: 0.9520\n",
      "\n",
      " 테스트 정확도: 0.9520\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate([Xstest_stop, Xqtest_stop], Ytest_stop)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "statistical-framework",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABOf0lEQVR4nO3dd3hUVfrA8e87k0nvlYRQQodACBARBSliAVSwIGLvrG0Vf+5adl3rurq7FtRVFBUrK4tgYV3sIkUEAUEIPZgASQgpkN6T8/vjDhgwQCBlksn7eZ55MrfNfc8Q7ptz7rnniDEGpZRSqrWxuToApZRSqj6aoJRSSrVKmqCUUkq1SpqglFJKtUqaoJRSSrVKmqCUUkq1SpqglFJKtUqaoJRqIBH5TkQOiIiXq2NRqj3QBKVUA4hIV+AMwAATW/C8Hi11LqVaG01QSjXMNcBK4C3g2oMrRaSTiHwoIjkikici/6qz7WYR2SIiRSKyWUQGO9cbEelRZ7+3ROSvzvejRSRdRO4TkSzgTREJEZFPnec44HwfW+f4UBF5U0Qynds/dq5PFpEL6uznEJFcEUlspu9IqSalCUqphrkGmON8nSsiUSJiBz4FdgFdgY7AXAARuRR4xHlcIFatK6+B5+oAhAJdgGlY/0/fdC53BsqAf9XZ/13AF4gHIoHnnOvfAa6qs98EYK8xZn0D41DKpUTH4lPq2ERkBLAYiDbG5IrIVuBVrBrVQuf66iOO+QJYZIx5vp7PM0BPY0yKc/ktIN0Y86CIjAa+BAKNMeVHiScRWGyMCRGRaCADCDPGHDhivxhgG9DRGFMoIvOBH40x/zjJr0KpFqU1KKWO71rgS2NMrnP53851nYBdRyYnp07AzpM8X07d5CQiviLyqojsEpFCYCkQ7KzBdQL2H5mcAIwxmcD3wCUiEgyMx6oBKtUm6A1YpY5BRHyAKYDdeU8IwAsIBvYBnUXEo54ktQfofpSPLcVqkjuoA5BeZ/nIZo17gN7AqcaYLGcNah0gzvOEikiwMSa/nnO9DdyE9X/9B2NMxlFiUqrV0RqUUsd2IVAD9AMSna++wDLntr3AUyLiJyLeIjLcedzrwB9EZIhYeohIF+e29cAVImIXkXHAqOPEEIB13ylfREKBhw9uMMbsBT4DXnZ2pnCIyMg6x34MDAbuwronpVSboQlKqWO7FnjTGLPbGJN18IXVSeFy4AKgB7AbqxZ0GYAx5gPgCazmwCKsRBHq/My7nMflA1c6tx3LDMAHyMW67/X5EduvBqqArUA2MP3gBmNMGbAAiAM+bHixlXI97SShlJsTkYeAXsaYq467s1KtiN6DUsqNOZsEb8SqZSnVpmgTn1JuSkRuxupE8ZkxZqmr41HqRGkTn1JKqVbpuDUoEZktItkiknyU7SIiL4hIiohsODici3PbOBHZ5tx2f1MGrpRSyr0dtwbl7LJaDLxjjOlfz/YJwO+xhlE5FXjeGHOq8yHC7cDZWL2bVgOXG2M2Hy+o8PBw07Vr1xMsilJKqbZo7dq1ucaYiCPXH7eThDFmqXMk56OZhJW8DLBSRIKdw690BVKMMb8AiMhc577HTVBdu3ZlzZo1x9tNKaWUGxCRXfWtb4pOEh2xbsQelO5cd7T1RwtwmoisEZE1OTk5TRCWUkqptqwpEpTUs84cY329jDGzjDFJxpikiIjf1PSUUkq1IsYYcktzj79jIzTFc1DpWANWHhQLZAKeR1mvlFLtTq2pRRBE6vvb/cQdKDvAnsI9+Dn8CPAKwMfDBxFBEKpqqyisKKSgvIB9JfvYlb+L3QW7qTE1BHsHE+wdjDGG0qpSSqpKSC9M55cDv5BemE64bzg9QnvQKbATOaU57CrYRXZJNpF+kXQK7ISfw4+N2RtZl7WO/WX7KX6gGC+P5plkuikS1ELgDuc9plOBAmPMXhHJAXqKSBzWdABTgStO9iRVVVWkp6dTXl7vDATqBHl7exMbG4vD4XB1KEq1KjW1NaTsT2FzzmZyS3PZX7afkqoSInwjiAmIIco/igDPAPw9/bGJjaziLLKKsyiuLMbX4Yufpx/FlcXs3L+TlP0ppOankpafxu6C3fg4fOgd1pve4b2pqqkisyiTrOIsRAQ/hx8+Dh/sYgfAJja8PLzwsnvh5eGFw+bAbrNTWlXKur3rSM1PPaFy2cRqMKs1tb/ZFuIdQreQbvQI7UFOaQ6fbv+UfSX7CPIKoktwFyL9ItlTsIcf9vxAYUUh/SL6MaHnBAZ1GER1bTVeNE+CakgvvveB0UA41ujNDwMOAGPMK2L9OfAvYBzWKM3XG2PWOI+dgDWOmB2YbYx5oiFBJSUlmSM7SaSmphIQEEBYWFiT/QXSXhljyMvLo6ioiLi4OFeHo9Qx1ZpaCsoLMBgEocbUUFBeQH55PlnFWezYv4OU/SlkFWdRVl1GeXU5HjYPIv0iifSNxM/TDwBB2F+2n4yiDDKLMskvz6e4spiSqhI87Z4EeAbg7eHNzgM7Ka9umj+EI/0iiQuOIy4kji5BXSipLGFr3la2523Hy+5FTEAMHfw7AFBaVUppVSm1phaDodbUUllTSUmRB7lrR+ARtQOPjhtwOISEqAQGdxhChOnH9s0+7NjkR+4+L8BggODwMpJG5dAvvoYo/0i6BHehY0BHPGweLFtZyuw3a0lNcVBwwIOCAht2mw0fH/Dzg549ITERunavIn2Xg59/hh07wOEAX1/w9jaHXYPnzAFPz8Z9TyKy1hiT9Jv1rfFB3foS1JYtW+jTp48mpyZijGHr1q307dvX1aGoNqSyppKMwgxCfUIJ9ApERDDGUFlTSXVtNb4OX0SEksoSNmZvZH3WejIKMyioKKCwohARwd/hj5+nH34OP/w9/fF1+FJQUUB6TgE7tnpj75BMmS2XgvICsoqz2Feyj+raOrOZ1HiA/fDZTQJqOxHl0ZvAiAJ8vbyoqK4gpzSH7JJsyqrKMLk9YPVteAXnE3fO/+gUEUKIdyjZq8bw87xJGGPwCs3FMzgXf48A/CScIO9Afj+9irNG+uPj8CG3NJdf9u3j008NP6/1Ycv6YHIz/QgKriE0VIiONnTsWkqHzsV071nD6MExdInxByAnx7rI+/tD376/XtDz8iA5GcrKrGUPDxg+HHx8rOU9e2DCBGsfgMBASEiA3FxIT4fi4jrfQQDYbGAMFBZa6zp1glNOgbAwCA6Gb7+FtWutz09IsNaHhFjHlJZCURFs2QKZdW7GRERAnz6/7nNkI9ZPP4FXIytQR0tQbWosPk1OTUe/S1VXUUURP+/7mU3Zm6isqTy0XkSwiY2C8gKW7FrC0l3LKCsR8CrBYXPg6/CluLKEmrRTIfMUqPLFURtElWc2xP4A0WsRRyWBXoEEeQdhjKEw34OiVZdQW+wAKqzJTNJHQOZQqPXA5lVK2JDv6DJyGWMG5dEpLJwI3wjSNsSyaNYwUn7qRERsEd37FhMabCdlQyjbt3lQhHXh797dqgWM6mW9X7wYPvjAqgFUVELu2ke48i746itYutSqLXTvDunp0exNg3I72HxhVzZcfC7cfTf8+c/w77cjefLJSLKzrQt8UhKMSoKCAivRpGyGr/4XQHV11KHvLzjYurAXFPz6XTsc0Ls3HDgAGfXMzhUZCf/3fzBiBFx2mZVsPvwQKiutBLN5M/TvD+PGQZcuVqIZONBKNgft3QuLFlmvrVth/34rxt694cUX4aqrrNiOJicHUlIgLg46dDiZ36im0aZqUPrXftPS79R9VNdWU11bjafdE5vYqKiuOHRvxG6zE+kXSYRvBDsP7OT73d+zMmMlmUWZHCg7QG5pLmn5ac7GIafiSNh1BjhKwTcPqr0I3X0jlRsvoCQ3hNi+WXRJSka8itn8xXDydkceOlRstZha636Hw2FISoIzzxTOOAO++QZmzrT+8vfyMs79DQkJhrPG2klMhC+/hHnzrAuzh4eVQHx9rWQSFWVdXHfvhp9/hvx8q4YwbJi1LSUFtm+3aispKVBRYdU6br8dpk+H1FR46CHrHOHh8Le/wQ03gN3+2++0sBDuvRdefdXaXlMDZ54Jf/mLVcup7/ZtVRWkpVnn377deolYiaFHDytRbdgAGzdaNZeBA60EExhoHZ+XZyWQL76wljt2tJJMQkJjf0OsRNla/y51iyY+V15M8/Pz+fe//81tt912QsdNmDCBf//73wQf688VF3H1d6rqt694H9+kfsP6rPX0CO3BoA6DiA6IZnXGapbvXk5yTjJ5BeX88skVFG09hVr/DGr9d0HYDohdiS16A7X2Y9xDKQkjuHAkIRWDsBd1gaIYIoL86dExhI7BUSz9xo8flntgzOFXM09POPdc62L59dfw44/WRW/oUPjd72DSJKuZyeGAfftg1SpYscJKLKtXWxd4m82qFdx//7EvumVl1jl++MF6pafDtGlw223WfZKGqK21jgsJseKqa9s2q2YQFHT8z/nmG5g7F664AsaMadi5G2vtWitJ33GH1Uzn7jRBNVJaWhrnn38+ycmHD0lYU1ODvb4/v9oAV3+nbV1V1a8XurCw3/51mpVlNS3Z7TBiZCX7fJZyoHw/NrFhjOGXA7+QnJPMxl+yKcmOoDq/AyXZEeSkRsG+gXCgG/jmQGA6BKdBx9U4Ov9EF4+hZMx9gLLcDkT1TaGmzI+inBAqSrwBsDuq8A8ppabSk6oKBw7PWnwCyvD0L6EkL5j8HN/D4gwLs+4rlJRYy/36weTJ1r0PY6y/6qurYfTowy/o2dlWDaZXr+N/V4WFVsLq3h26dTvpr1y5Kbe4B+VK999/Pzt37iQxMRGHw4G/vz/R0dGsX7+ezZs3c+GFF7Jnzx7Ky8u56667mDZtGvDrsE3FxcWMHz+eESNGsGLFCjp27Mgnn3yCz8G7oapNqayEs86CZcusZS8v635Az56GsNj9bNpSzU/LIw41dYEn+PeD6J8gbLuVcHL64bH7Eapzuh/22QGhxcQn1DA4PoA9WXZSUoNJTzuFop+vowpIwbpp/eoCGDmyx6HjMjJg5UpYudJBbm4Qvr7WvZKKCsjL82T//iAikqwms4QEq8kpJubXG9wHk1TdexnHEhlpvRoiMBDOPrth+yp1UJusQU3/fDrrs9Y36TkTOyQyY9yMo26vW4P67rvvOO+880hOTj7UTXv//v2EhoZSVlbGKaecwpIlSwgLCzssQfXo0YM1a9aQmJjIlClTmDhxIldd5bpJTrUGdbiMDKv9v3t3616D9Ze+Ib+kjNziA5TLfg6UHyC7OIenH+jDqv/Gk3TVx/j6VVOWF8HedC+ydgVQnRMHvrkw8F0Y+A5RAeH0LroVSRvD3l/C2J3qSXmZjYAAw6hRwujRVsKJjbVe9SUIY6z7LitXWknkyisb33NKqdZCa1BNbOjQoYc9Q/TCCy/w0UcfAbBnzx527NhB2BFXmri4OBITEwEYMmQIaWlpLRWuOo4VK+Dii617JwfZfPdTW+0Jlf5gc0DCFzDiKdgxAb64BPuoJ9kz8Hkqayop71RO6GmhXNZ1NKO7jGFA1ADCfacS4nMrId4hh/WarK21msfCwwWPBv4PFLFqaF26NHHBlWrF2mSCOlZNp6X41blT+9133/H111/zww8/4Ovry+jRo+sd8cKrzp+8drudsoMPP6hmt2mTddO5tNSqoQQGV+HhXYbNUcnmHRW88EQHPEIy4dbxYKsmMPMiIorOISTQg9AwQ0V+GCsWXkv1z9cjAuMnVvDJh/djtz9wwrHYbK7tuqtUW9EmE5QrBAQEUFRUVO+2goICQkJC8PX1ZevWraxcubKFo2ufamqsjgre3vVvLy01PPFiBu+/40Pq5jDEVgP2akyVF9ZgKHX6CXf/goG//yeXDrmK83qeR//I/r95Viw7G2bMsLovz57tVW/XZKVU09EE1UBhYWEMHz6c/v374+PjQ1TUrw/jjRs3jldeeYWEhAR69+7NsGHDXBip+8vIgDfegNdft554HzoUBp+eT0hcKna/fGq98/h8kbD2o5HUFsdCh59g3OPEnL6Unp2D6OTTmwjpg09tOLYaP3w8fLni7Hi6hHx9zPNGRlrPzSilWkab7CShmkZb+06Ngccfh8ces2pPA07Lguh1bF8bRcXuBDCH/70VNXA91/9+L9dM7Eq3kG7NNuKyUqpxtJOEajOMscYay8uDmK5FLNn1HavSf+S//xrFho/PIviUz8g/7Q42hv5CsHcw46eM5vSIcURUnEpNSTCVxQEkDQjklCGJQKKLS6OUOlmaoJRLLVsGr71mJaSyMuuBzp07DQUFzvs/ofsgYQ0UdoGfziJ41DsMvu5dxna/kbO6ncWQ6CHYbXozSCl3pAlKucTXX8Ojj8Ly5eAdUEJoTD5B/p74+Ru8By+jwGspkUGB+Gy7lt1LHsEY4YEH4IknrkHkGleHr5RqAZqgVIt75hn4wx8gMqacgAsfp6z/v8hyFJPpnEitY0BHXh/9KNcmXouHzYM9e6zBN88808WBK6ValCYo1WKMgYcfqebxxzzoP2YLW85Iolt4Rz687Hu6h3Rnc85mMooyOKvbWfg6fh0vrlOn9jFgplLqcJqgVItIyd7D6Kt/IOPLKZD4Jsln3MSkvhfw9oVvE+RtjUA6JGYIQxji4kiVUq2F7fi7gIiME5FtIpIiIvfXs/2PIrLe+UoWkRoRCXVuSxORjc5ta3776e7J39+aSTMzM5PJkyfXu8/o0aM5sjv9kWbMmEFpaemh5QkTJpCfn99kcbaEGW+n0KdfNRlfTmHwpBW895Y3ybdv4KPLPjqUnJRS6kjHrUGJiB14CTgbSAdWi8hCY8zmg/sYY/4J/NO5/wXA3caY/XU+ZowxJrdJI28jYmJimD9//kkfP2PGDK666ip8fa0mr0WLFjVVaM3CGGvK6FWrYM1PVXyx5AA7N/bAIzKFV/+TxrQppwOnuzpMpVQb0JAa1FAgxRjzizGmEpgLTDrG/pcD7zdFcK3Jfffdx8svv3xo+ZFHHuHRRx9l7NixDB48mAEDBvDJJ5/85ri0tDT69+8PQFlZGVOnTiUhIYHLLrvssLH4br31VpKSkoiPj+fhhx8GrAFoMzMzGTNmDGOcM6V17dqV3Fwr1z/77LP079+f/v37M2PGjEPn69u3LzfffDPx8fGcc845LTLm36pV1uRqcXGG+HhrltKXZ1Ww88BOOk95jtStAUyb0rXZ41BKuY+G3IPqCOyps5wOnFrfjiLiC4wD7qiz2gBfiogBXjXGzDrJWA+ZPh3Wr2/spxwuMdEaZ+1opk6dyvTp0w/NqDtv3jw+//xz7r77bgIDA8nNzWXYsGFMnDjxN2O4HTRz5kx8fX3ZsGEDGzZsYPDgwYe2PfHEE4SGhlJTU8PYsWPZsGEDd955J88++yyLFy8mPDz8sM9au3Ytb775JqtWrcIYw6mnnsqoUaMICQlhx44dvP/++7z22mtMmTKFBQsWNNu0Hjt2wAMPwIIF4OtrCI3/CQa8StdBu7j49AQm9BrHyC534LDXMz+2UkodQ0MSVH1X26ONj3QB8P0RzXvDjTGZIhIJfCUiW40xS39zEpFpwDSAzp07NyCsljVo0CCys7PJzMwkJyeHkJAQoqOjufvuu1m6dCk2m42MjAz27dtHh6MMVb106VLuvPNOABISEkioM+f1vHnzmDVrFtXV1ezdu5fNmzcftv1Iy5cv56KLLjo0qvrFF1/MsmXLmDhxYotM61FdbT3H9NRT1rxEN9+TznfRE9lZ8jOPjXqEP50xUx+gVUo1SkMSVDpQt5NvLJB5lH2nckTznjEm0/kzW0Q+wmoy/E2CctasZoE1Ft+xAjpWTac5TZ48mfnz55OVlcXUqVOZM2cOOTk5rF27FofDQdeuXeudZqOu+mpXqampPP3006xevZqQkBCuu+66437OscZQbO5pPfbtg8svh8WL4fTzt1M66i5eK/mcKBPF11d/zZi4MU16PqVU+9SQe1CrgZ4iEicinlhJaOGRO4lIEDAK+KTOOj8RCTj4HjgHSG6KwF1h6tSpzJ07l/nz5zN58mQKCgqIjIzE4XCwePFidu3adczjR44cyZw5cwBITk5mw4YNABQWFuLn50dQUBD79u3js88+O3TM0ab5GDlyJB9//DGlpaWUlJTw0UcfccYZZzRhaQ+Xm2uN/vDMMzBwUDVLv6/CZ/JtrEjqTaXvbp4f9zxb79iqyUkp1WSOW4MyxlSLyB3AF4AdmG2M2SQitzi3v+Lc9SLgS2NMSZ3Do4CPnLUGD+DfxpjPm7IALSk+Pp6ioiI6duxIdHQ0V155JRdccAFJSUkkJibSp0+fYx5/6623cv3115OQkEBiYiJDhw4FYODAgQwaNIj4+Hi6devG8OHDDx0zbdo0xo8fT3R0NIsXLz60fvDgwVx33XWHPuOmm25i0KBBzdKc98ILcNdddVZEbMV24zVccGZP7jhlKSM6jzjqfTellDpZOt1GO9aQ73TtWhg2DM46C8ZcsZb7No7noiEjeHH8i3QM7NhCkSql3JlOt6FOWHGxda+pQwf4x8wMzvxgHPFdo3j3onfx8/Q7/gcopVQjaIJSRzV9ujW9+RdfVfG7by6lvLqcBVMWaHJSSrWINpWgjDF6r6OJHK9p98svrWnVH3gAvuMRfkj/gXmT59E7vHcLRaiUau8aNBZfa+Dt7U1eXt5xL6zq+Iwx5OXl4e3tfdR9nnsOYmLg4ls28I8V/+C6xOu4NP7SFoxSKdXetZkaVGxsLOnp6eTk5Lg6FLfg7e1NbGxsvdt27oTPP4e/PFTLrV/cSKhPKM+c80wLR6iUau/aTIJyOBzExcW5Oox24dVXwW4HGfI6a9at4T+T/0OoT6irw1JKtTNtpolPtYyyMuve09kTSvjnxulc0OsCLu2nTXtKqZanCUod5oMPYP9+KE78ByLCSxNe0o4pSimX0ASlDjNzJnTqVspy22PcP/x+OgXpXOtKKdfQBKUO+eorWLkSTNLLxAbFcs/p97g6JKVUO9ZmOkmo5pWcDJdeCjFx+aR3f5T3xr6Cr8PX1WEppdoxTVCKzEyYMAG8fWqpuvxshsb14/IBl7s6LKVUO6cJqp0rKbGS04EDhqEP3st3FT/xybnLsYm2/iqlXEuvQu3c9OmwYQOc96d3+Lb8GZ4++2lO63Saq8NSSilNUO3ZggXw+utw/g3J/KfyOm4adBPTh013dVhKKQVoE1+7tWcP3HyzoXPffXze6VRGdRnFS+fpM09KqdZDa1Bu7qefYMeOw9elpcFFU0opKC1j91kjOKvHKBZMWYCn3dMlMSqlVH20BtUKVVbC8uWwcSMkJMApp4C//+H7VFXB449bPfBefBF8fA7f/v338Je/wMFZ4k8dZjjr/P0s+vYA6xbHgXjie+ltzLzxIa5OuFprTkqpVqdBCUpExgHPA3bgdWPMU0dsHw18AqQ6V31ojHmsIceqX+3fD3fcAf/7HxQW/rreZoMhQ+D66+HKKyEvz5rpdtUqa/uqDQe45elFVHvmkpcexkczRpD8fVe8gwroNvUd9pcWsmrlRFY9OAC8bYSOnc3lN+znz+c/SnRAtGsKq5RSxyHHm19JROzAduBsIB1YDVxujNlcZ5/RwB+MMeef6LH1SUpKMmvWrDnRsrR5994LzzwDN9wA558PSUlWD7sVKwwfflLJ5o1eOLwrqKWaWqoxF9wIxgYfvgcRW6D7l7DyLvAohzP+Roex8+nZIYauwV3pFNgZn6L+jE9MZEjXPq4uqlJKHSIia40xSUeub0gNaiiQYoz5xflBc4FJwDGTTBMc2+rl5UF+PnTv/uu64mJ44gno0weuvbb+41auhNtvt5roJkyA8upyyoq8mTkTLrsMXp1Vy8dbP+b2Ze+Qsj+FVJ9USi8uhVNPoXbd7QTTmTHTFpHYZxBxIXFkTdrGX24ZQOmKgUy9qpz7Hy6jV5eH8XFoZVUp1XY1JEF1BPbUWU4HTq1nv9NE5GcgE6s2tekEjkVEpgHTADp37tyAsJpXZSVs3QqBgRAaCgEBUPc2TWYmnHEGpKZazW2PPAK7dsFNN1k/AcrL4aKrsrGJjXDfcADmzoXrrjNUVAhTrioi7sHzSC5aRsTa5ygunk7fSQsZMuth1metp3NQZwZ1GMTZ3c6mZ1hPTos9jQFRA/CweQBjfg1mAExIss6XmOgNHH2mXKWUajOMMcd8AZdi3Ts6uHw18OIR+wQC/s73E4AdDT22vteQIUOMq6xZY8wddxgTFmYM/Prq0MGYd981prbWmNxcY+LjjfH3N+a224zx9TXGZrP269XLmMWLjRk/odqAMfYLbzQ8jIm+b6zpfsF86/O6LDVcPdZgqzJRIxaZP3z6kHEE7Df0XGR4BNPjhR7mnfXvmKqaKpd9D0op1VKANaaeXNCQGlQ6UHfOhVisWlLdJFdY5/0iEXlZRMIbcmxr8t13MGYMeHnBhRfCBRdYNam8PJg/H66+Gt56CwoKDDtSDFf9fQ4Fnb5gxAAbWz49By/vWs64fDVLCGfzmPdh2/PUfDIL/yXPs/eAHwAdRnzO1X/6nuFxv2dJh2qe++d4dr09nqoi+N+HIwjqtZxTY0911pKUUqr9ashVcDXQU0TigAxgKnBF3R1EpAOwzxhjRGQo1vNVeUD+8Y5tTRYsAF9f6yFW8TnAOz+/w+aczWyVreRMzSSy6xUsXngPtRU+cNlFzM7/H3HEEewdTNzkXZRVlbHwlzRyknPoG96Xz//ry0fP2ygu9uPMM63kFxc3DhgHwLmPwf8+sSYJPP10GD82AJHhrv0SlFKqlThugjLGVIvIHcAXWF3FZxtjNonILc7trwCTgVtFpBooA6Y6q231HttMZWm0L76A0aMho2ojF753Ib8c+IVQn1D6hvfl1E6nUBu7jaKxd+EojWXKmVdwdrc3ifCL+M3nlFaV4uPhg4hw7itHP5+3N8yebdXWHn308HtcSinV3h23m7kruKKbeWoqdOsG192/nnkBwwnyCuKDSz9geOfmr9HU1IDd3uynUUqpVqkx3czbhS+/tH6+VTSV03oMZP6U+cQExLTIuTU5KaXUb2mCclr0eRX24Gzi471ZfO1ivDy8XB2SUkq1azpYLFBdDV98VU1Nt8949YJXNDkppVQroAkKmPPZDipKfDjr7FqGxQ5zdThKKaXQBEWtqeXB15aC1PDq76e4OhyllFJO7TpBVVRXcPmCy0lf14/u/ffTrWOwq0NSSinl1G47SRRWFHLRfy7i23U7kMz3ueJ6fQhJKaVak3aToIyB556DoCA457xSJn48mg1fJuL79SJqPW1M0dY9pZRqVdpNgtq5E+65x3pvs3tRGzIHcvsyaLg1mkOvXq6NTyml1OHazT2oZcusn3Pm1BI05nX8Aqt47jnDkiWanJRSqjVqNzWoZcsgPBx8Bi3kwI5bmPvcXC7rn+DqsJRSSh1Fu6lBLV0KI0bAsyufoUtQFy7pd4mrQ1JKKXUM7SJB7d1r3YPqPGAXy3cvZ/qw6TrfklJKtXLt4ip98P7TJp9XCJIgbhx0o2sDUkopdVztoga1bBn4+tXybfkz3Dz4ZgK8AlwdklJKqeNoFwlq6VLoEp+JsVVx9cCrXR2OUkqpBnD7BJWfDxs3QmXst/QI7cGAyAGuDkkppVQDuH2C+v57axSJ1MB3uKTvJYjOq66UUm1CgxKUiIwTkW0ikiIi99ez/UoR2eB8rRCRgXW2pYnIRhFZLyItO487VvOe3aOG2pgVXNz34pY+vVJKqZN03F58ImIHXgLOBtKB1SKy0Bizuc5uqcAoY8wBERkPzAJOrbN9jDEmtwnjbrBlyyAwbjv+4eGcEnOKK0JQSil1EhpSgxoKpBhjfjHGVAJzgUl1dzDGrDDGHHAurgRimzbMk7NvH6xebSiMXsjFfS/W5j2llGpDGpKgOgJ76iynO9cdzY3AZ3WWDfCliKwVkWlHO0hEponIGhFZk5OT04Cwju+996C6WqgZ8BaX9NWRI5RSqi1pyIO69VU7TL07iozBSlAj6qwebozJFJFI4CsR2WqMWfqbDzRmFlbTIElJSfV+/okwBt54A8J6bcOj6wFO73R6Yz9SKaVUC2pIDSod6FRnORbIPHInEUkAXgcmGWPyDq43xmQ6f2YDH2E1GTa7VatgyxYo7DeDi/pchN1mb4nTKqWUaiINSVCrgZ4iEicinsBUYGHdHUSkM/AhcLUxZnud9X4iEnDwPXAOkNxUwR/L7Nng8K6kqu973D709pY4pVJKqSZ03CY+Y0y1iNwBfAHYgdnGmE0icotz+yvAQ0AY8LKzI0K1MSYJiAI+cq7zAP5tjPm8WUpSR0kJzJ1rsPf/kLHxI+gf2b+5T6mUUqqJNWiwWGPMImDREeteqfP+JuCmeo77BRh45PrmtmABFBUJDHiJP57+aEufXimlVBNwy9HMZ882OCJ2ET+0hDFdx7g6HKWUUifB7RJUTQ0Edkmlyu8Z7h3+R332SSml2ii3S1B2OxSMuZ7O+WlM7jfD1eEopZQ6SW6XoKprqxnWcRhXDbgKh93h6nCUUkqdJLdLUB42D/5+9t9dHYZSSqlGcvvpNpRSSrVNmqCUUkq1SmJMo4e9a3IikgPsauTHhAMumeKjhWk53U97KauW0700ppxdjDERR65slQmqKYjIGudoFm5Ny+l+2ktZtZzupTnKqU18SimlWiVNUEoppVold05Qs1wdQAvRcrqf9lJWLad7afJyuu09KKWUUm2bO9eglFJKtWGaoJRSSrVKbpegRGSciGwTkRQRud/V8TQVEekkIotFZIuIbBKRu5zrQ0XkKxHZ4fwZ4upYm4KI2EVknYh86lx213IGi8h8Ednq/Lc9zR3LKiJ3O39vk0XkfRHxdpdyishsEckWkeQ6645aNhF5wHl92iYi57om6hN3lHL+0/m7u0FEPhKR4DrbGl1Ot0pQImIHXgLGA/2Ay0Wkn2ujajLVwD3GmL7AMOB2Z9nuB74xxvQEvnEuu4O7gC11lt21nM8Dnxtj+mBN7rkFNyuriHQE7gSSjDH9sWbmnor7lPMtYNwR6+otm/P/7FQg3nnMy87rVlvwFr8t51dAf2NMArAdeACarpxulaCAoUCKMeYXY0wlMBeY5OKYmoQxZq8x5ifn+yKsC1lHrPK97dztbeBClwTYhEQkFjgPeL3OancsZyAwEngDwBhTaYzJxw3LijUwtY+IeAC+QCZuUk5jzFJg/xGrj1a2ScBcY0yFMSYVSMG6brV69ZXTGPOlMabaubgSiHW+b5JyuluC6gjsqbOc7lznVkSkKzAIWAVEGWP2gpXEgEgXhtZUZgD3ArV11rljObsBOcCbzubM10XEDzcrqzEmA3ga2A3sBQqMMV/iZuU8wtHK5s7XqBuAz5zvm6Sc7pag6ps+16360YuIP7AAmG6MKXR1PE1NRM4Hso0xa10dSwvwAAYDM40xg4AS2m4z11E5779MAuKAGMBPRK5ybVQu45bXKBH5M9ZtiDkHV9Wz2wmX090SVDrQqc5yLFZTglsQEQdWcppjjPnQuXqfiEQ7t0cD2a6Kr4kMByaKSBpWE+2ZIvIe7ldOsH5f040xq5zL87ESlruV9Swg1RiTY4ypAj4ETsf9ylnX0crmdtcoEbkWOB+40vz6YG2TlNPdEtRqoKeIxImIJ9ZNuoUujqlJiIhg3avYYox5ts6mhcC1zvfXAp+0dGxNyRjzgDEm1hjTFevf71tjzFW4WTkBjDFZwB4R6e1cNRbYjPuVdTcwTER8nb/HY7HuobpbOes6WtkWAlNFxEtE4oCewI8uiK9JiMg44D5gojGmtM6mpimnMcatXsAErN4kO4E/uzqeJizXCKwq8gZgvfM1AQjD6iW0w/kz1NWxNmGZRwOfOt+7ZTmBRGCN89/1YyDEHcsKPApsBZKBdwEvdykn8D7WvbUqrJrDjccqG/Bn5/VpGzDe1fE3spwpWPeaDl6TXmnKcupQR0oppVold2viU0op5SY0QSmllGqVNEEppZRqlTRBKaWUapU0QSmllGqVNEEppZRqlTRBKaWUapU0QSmllGqVNEEppZRqlTRBKaWUapU0QSmllGqVNEEppZRqlTRBKaWUapU0QSnVTEQkTUTOcnUcSrVVmqCUUkq1SpqglGpBzhlGZ4hIpvM1Q0S8nNvCReRTEckXkf0iskxEbM5t94lIhogUicg2ERnr2pIo1fw8XB2AUu3Mn4FhWDPpGqypwB8E/gLcgzVTaYRz32GAcU4JfwdwijEmU0S6AvaWDVuplqc1KKVa1pXAY8aYbGNMDtZU6Fc7t1UB0UAXY0yVMWaZsaa8rsGaIr2fiDiMMWnGmJ0uiV6pFqQJSqmWFQPsqrO8y7kO4J9ACvCliPwiIvcDGGNSgOnAI0C2iMwVkRiUcnOaoJRqWZlAlzrLnZ3rMMYUGWPuMcZ0Ay4A/u/gvSZjzL+NMSOcxxrg7y0btlItTxOUUs3LISLeB1/A+8CDIhIhIuHAQ8B7ACJyvoj0EBEBCrGa9mpEpLeInOnsTFEOlDm3KeXWNEEp1bwWYSWUgy9vYA2wAdgI/AT81blvT+BroBj4AXjZGPMd1v2np4BcIAuIBP7UYiVQykXEugerlFJKtS5ag1JKKdUqaYJSSinVKmmCUkop1SppglJKKdUqtcqhjsLDw03Xrl1dHYZSSqkWsHbt2lxjTMSR61tlguratStr1qxxdRhKKaVagIjsqm+9NvEppZRqldwuQdXU1jA3eS5fpHzh6lCUUko1Qqts4msMm9h4dMmjBHoFcm6Pc10djlJKqZPkdglKRLgt6Tbu/PxO1mSuISkmydUhKaXaoKqqKtLT0ykvL3d1KG7D29ub2NhYHA5Hg/ZvlUMdJSUlmcZ0kigoL6Djsx2ZEj+F2ZNmN2FkSqn2IjU1lYCAAMLCwrDG71WNYYwhLy+PoqIi4uLiDtsmImuNMb+pTbjdPSiAIO8grkq4iveT32d/2X5Xh6OUaoPKy8s1OTUhESEsLOyEaqRul6CqquDaa8Fn1cOUV5fz5ro3XR2SUqqN0uTUtE70+3S7BOVwQF4evP1SNMPCz2XmmpnUmlpXh6WUUuoEuV2CAnjsMThwADpuepqdB3by5c4vXR2SUkqdkPz8fF5++eUTPm7ChAnk5+c3fUAu4JYJavBguOQS+PK9eKKkH08uf5LW2BlEKaWO5mgJqqbm2JMpL1q0iODg4GaKqmW5ZYICePRRKC4WEna+w9JdS/lipz64q5RqO+6//3527txJYmIip5xyCmPGjOGKK65gwIABAFx44YUMGTKE+Ph4Zs2adei4rl27kpubS1paGn379uXmm28mPj6ec845h7KyMlcV56S43XNQB8XHw+WXw8fzB9P5gVN44JsHOKf7OdjEbXOyUqqZTP98Ouuz1jfpZyZ2SGTGuBlH3f7UU0+RnJzM+vXr+e677zjvvPNITk4+1EV79uzZhIaGUlZWximnnMIll1xCWFjYYZ+xY8cO3n//fV577TWmTJnCggULuOqqq5q0HM3Jra/WDz8MFRVC783vsD5rPR9s+sDVISml1EkZOnToYc8PvfDCCwwcOJBhw4axZ88eduzY8Ztj4uLiSExMBGDIkCGkpaW1ULRNw21rUAC9esGNN8Ls2b3p+eAE/rL4L1zc92Ic9oY9xayUUsAxazotxc/P79D77777jq+//poffvgBX19fRo8eXe/zRV5eXofe2+32NtfE59Y1KLDuRXl5CRErXmPH/h3MXDPT1SEppdRxBQQEUFRUVO+2goICQkJC8PX1ZevWraxcubKFo2sZbp+gOnSAe++FFV/GcGrt3dz39X1sydni6rCUUuqYwsLCGD58OP379+ePf/zjYdvGjRtHdXU1CQkJ/OUvf2HYsGEuirJ5ueVYfEcqKYGePaFjp0pSL+lIp6BYVt64Ei8Pr+MfrJRql7Zs2ULfvn1dHYbbqe97bZax+ERktohki0jyUbaPFpECEVnvfD3UmPOdLD8/6+HdNT96krhqLet/9OXBb1wSilJKqQZqbCeJt4B/Ae8cY59lxpjzG3meRrv+eli1Ct57rzOUf8/TC3axsG8Kw/p0Ia6Lgz/8Afz9XR2lUkqpgxpVgzLGLAXaxHDhdju89hpkZ8NrsyuI7p3J9l35vPdRDo8+Ci/P1PH6lFKqNWmJThKnicjPIvKZiMS3wPmOKSAAbrrei8xVp7F8ZQXDn5sKnZfy+DM5VFZXuzo8pZRSTs2doH4CuhhjBgIvAh8fbUcRmSYia0RkTU5OTjOHZRneeThLrlvCZdfvp3hfFGc9/Heqaqpa5NxKKaWOrVkTlDGm0BhT7Hy/CHCISPhR9p1ljEkyxiRFREQ0Z1iHERHe+dOF+IeUsGxBf6bMn0J5tU7xrJRSrtasCUpEOohzhioRGeo8X15znvNkeHrCnbf6ITsu4OOVPzHyzZGkF6a7OiyllDoh/s6eXpmZmUyePLnefUaPHs3xHuOZMWMGpaWlh5ZdNYVHY7uZvw/8APQWkXQRuVFEbhGRW5y7TAaSReRn4AVgqmmND14B06aBYGNy2Vdsyd1C0qwklu9e7uqwlFLqhMXExDB//vyTPv7IBOWqKTwa24vvcmNMtDHGYYyJNca8YYx5xRjzinP7v4wx8caYgcaYYcaYFU0TdtPr0gXOPx+WftSLheetIcArgDPePIO+L/Xltv/dxgebPiC7OIdFi+DVV6F1plmllDu57777DpsT6pFHHuHRRx9l7NixDB48mAEDBvDJJ5/85ri0tDT69+8PQFlZGVOnTiUhIYHLLrvssPH4br31VpKSkoiPj+fhhx8GrEFoMzMzGTNmDGPGjAF+ncID4Nlnn6V///7079+fGTNmHDpfc0zt4daDxZ6oe+6BM8+Ec4b05vxJmzhzxEK2VX7GOz9+zMyPfoKvI2HXKADe/W8qs16voW9Ud5ytmCetvBy8vZuiBEqp5jB9Oqxf37SfmZgIzuv7UU2dOpXp06dz2223ATBv3jw+//xz7r77bgIDA8nNzWXYsGFMnDjxqNehmTNn4uvry4YNG9iwYQODBw8+tO2JJ54gNDSUmpoaxo4dy4YNG7jzzjt59tlnWbx4MeHhh3cZWLt2LW+++SarVq3CGMOpp57KqFGjCAkJaZapPTRB1TFyJGzbBjNnwhtveJK/YDJWK6XFP7SY2GufZ2fGAb7/3yPEn/41XDAOj/y+kDkYMR7Yo7Zi67CZuKgwzux4PsM7nE0n3z5UlNspL4fevSEuDkRg+3Z45BGYNw9eeQVuusllRVdKtUKDBg0iOzubzMxMcnJyCAkJITo6mrvvvpulS5dis9nIyMhg3759dOjQod7PWLp0KXfeeScACQkJJCQkHNo2b948Zs2aRXV1NXv37mXz5s2HbT/S8uXLueiiiw6NrH7xxRezbNkyJk6c2CxTe2iCOkL37vD009bQSEuXwr59kJMDXl5w/fX++PvfRXVtNU/9azcP/9+Z1L6QwqGnp6SWKmO1mm5yvl6s5xxBEUV0713Jzz+E4uUl9OkDt90GffrAiBEtU06lVMMdr6bTnCZPnsz8+fPJyspi6tSpzJkzh5ycHNauXYvD4aBr1671TrVRV321q9TUVJ5++mlWr15NSEgI11133XE/51hdCJpjag9NUEfh6wvjxtW/zcPmwYN3duasobBypVVVHzQIPDxsbNkCyclQWgo1HoVsL1hHTvUu8msyyS7PYOcWXwq2J/LTlkFwyjuEn/c+XWJ6kvrnvzNmgg8d/3AhFw1N4vpB15MQZf0lU1MDRUVwtHuUxsCyZeBwwGmnNcvXoZRykalTp3LzzTeTm5vLkiVLmDdvHpGRkTgcDhYvXsyuXbuOefzIkSOZM2cOY8aMITk5mQ0bNgBQWFiIn58fQUFB7Nu3j88++4zRo0cDv071cWQT38iRI7nuuuu4//77Mcbw0Ucf8e677zZLuUETVKMMG2a96kpKsl6WQGDUb47LLslmw74NrM2sZM3eLiRn/0T32/6PLU++yYHZb/HihseZEXY93SNisW+6mszlYynODaH34CzOvTiXSyZ5k9StOz4+wrJl8NBDsGSJ1Wz42GPwpz+Bze0nUlGqfYiPj6eoqIiOHTsSHR3NlVdeyQUXXEBSUhKJiYn06dPnmMffeuutXH/99SQkJJCYmMjQoUMBGDhwIIMGDSI+Pp5u3boxfPjwQ8dMmzaN8ePHEx0dzeLFiw+tHzx4MNddd92hz7jpppsYNGhQs83U2y6m22grFi2CKVOs6UEOkRo8en1NdeRq2DwZ8n79ZRR7NabGg6DwUi6/ZReZW2NZOD+A886Dp56yaoFeXnDgAKSnQ0YGBAZa98C6dYPQ0JYvo1JthU630TxOZLoNrUG1IhMmWMkkJQW2bIG8PDj/fDvR0edSUjmCrOJ9LPnhZ75ZVsLWjAx27sumwLGZgsQ3ecWjDOLBp+oPLPr0b/zvf8ef1v6BB+CJJ6yal1JKtTaaoFoZhwP69rVedfl5+tE9tBvdz4Mbzvt1fWlVKVnF97C3aC9bcrfw45AfWZZ4Gbu2BlNWXgs1XuCdD4HpSOBeBgSeQYLXJArWncmTTwZTUgLPPadNgkqp1kcTVBvn6/ClW0g3uoV0Y3jn4dw0+Ca4wNpWWlXK3qK97C7YTVp+Gjv27+CrX77ivcx3IB7C9r/NCy9cQ15BKW+/4Yvd7tqyKNXaGGMa/Zyj+tWJ3lLSBOXGfB2+dA/tTvfQ7ofW/W3s38gozODjrR/zdseXyKvazZy3HySzbBPfzI3X5j6lnLy9vcnLyyMsLEyTVBMwxpCXl4f3CYxKoJ0k2rktOVuYcO0m0j6bzKQ7VvDxi6e7OiSlWoWqqirS09OP+2yQajhvb29iY2NxOA6/R66dJFS9+kb0ZfPH3eg5eimf/GskV4V9xnuPjHd1WEq5nMPhIC4uztVhtGt6a1zh4+nFlq+HEdEvmTmPj+XF/37r6pCUUkoTlLIE+Hry87e9EXstDzyVQVFFkatDUkq1c5qg1CHRUQ7GTyqkZM2FPLDob64ORynVzjV2wsLZIpItIslH2S4i8oKIpIjIBhEZXN9+qvV46A+RUBnAy28Usm7vOleHo5Rqxxpbg3oLOMqQqgCMB3o6X9OAmY08n2pmQ4dCQmI19rW3M+2/v6OmtsbVISml2qnGzqi7FNh/jF0mAe8Yy0ogWESiG3NO1bxE4I7bPKjO6seaVQ7e3dB8IxUrpdSxNPc9qI7AnjrL6c51qhW74goIDDSEbXqQB799kNKqUleHpJRqh5o7QdX3+HW9TwaLyDQRWSMia3Jycpo5LHUsfn5wzTVC4bpzycio5fmVz7s6JKVUO9TcCSod6FRnORbIrG9HY8wsY0ySMSYpIiKimcNSxzN9Ogg2YlfP4cnlT5JTon80KKVaVnMnqIXANc7efMOAAmPM3mY+p2oC3bvDH/4A6cvGUJIyiL8u/aurQ1JKtTON7Wb+PvAD0FtE0kXkRhG5RURuce6yCPgFSAFeA25rVLSqRf3pTxAbCyGL3+OlH19hxZ4Vrg5JKdWONGosPmPM5cfZboDbG3MO5Tp+fvDsszBlSifCk+9n8rzJrJ22lugA7YiplGp+OpKEOqbJk+HMM6Hiy4fYn5zEpR9cSmVNpavDUkq1A5qg1DGJwOuvQ+dOdireWsj3b0zk9/+954QnHlNKqROlCUodV1wcrF4Nt9wCrLiXWbfcyHl/eZ3qmlpXh6aUcmOaoFSD+PjAzJmwYIEhyNaJz564mbAue5k3v9rVoSml3JQmKHVCLr5YyNkVymUPf0xhVR6XTbHx3n+KXR2WUsoNaYJSJ8zhEOY+ciH/+nAN0nE111xt593Ptrg6LKWUm9EEpU7a7cNv4NOFHtgCcrhmSjBPfjrH1SEppdyIJijVKBMGDWHpl4F41ATypxsHcP1/7tJu6EqpJqEJSjXa6UOC+fRjb2x58bz15/GMfuNssoqzXB2WUqqN0wSlmsS559h5bZYddo7jx9euYcDLCSzYvMDVYSml2jBNUKrJ3HCDNX5fzZobsX/zDybPncrlCy4nrzTP1aEppdogTVCqST3+ONx8M+z78jqi/p3O/K9/4fTZp7O7YLerQ1NKtTGaoFSTstlg1iz4+GOwlUZhXlvJrv9eyelvDGdr7lZXh6eUakM0QalmMWkSbN4MU6YIFV8+RN4Hf2XEG6P4YNMHVFRXuDo8pVQb0KjpNpQ6luBgmDMHunaFJ5+8FntJDFMKJxES6M1l8Zdx7/B7iQuJc3WYSqlWqrETFo4TkW0ikiIi99ezfbSIFIjIeufrocacT7U9IvC3v8GLL0Jp8llEv5tLQv6DvLX+bRJeSWDW2lk6MrpSql4nnaBExA68BIwH+gGXi0i/enZdZoxJdL4eO9nzqbbtjjvgm2+EkABfljz1f5z6XR69S27gd//9HePnjOeDTR+Qlp+myUopdUhjmviGAinGmF8ARGQuMAnY3BSBKfczZgysXw8vvwwPP+xDwTfP0yHuYRb3ep4v/r0EyjfgZw9h2s02Hp54PUHeQa4OWSnlQo1JUB2BPXWW04FT69nvNBH5GcgE/mCM2dSIc6o2zuGAu+6CG2+EefPg9ddDyfri0UPbS6SW55aUMvOcJ/nj3V5cMXAKvcN6IyIujFop5Qpysk0qInIpcK4x5ibn8tXAUGPM7+vsEwjUGmOKRWQC8LwxpudRPm8aMA2gc+fOQ3bt2nVScam2JzMTPDysThWZmXD1zQUs/zoIIpIhbjFhnXM4fagnfRILCfAMoFNQJy7qc5HWsJRyEyKy1hiT9Jv1jUhQpwGPGGPOdS4/AGCMefIYx6QBScaY3GN9dlJSklmzZs1JxaXaPmPgww/hib9XsGWzUF7iCYCt5+fUnnM3RGzF28ObS/pewtUJVzO662i8PLxcHLVS6mQ1R4LyALYDY4EMYDVwRd0mPBHpAOwzxhgRGQrMB7qY45xUE5Q6yBirVvWf/8Bjj0FxseGsC/aT7bGGLcXfU+6RhadfOYlduzKif2eSEgKIC+1Cv4h+BHoFujp8pVQDNHmCcn7oBGAGYAdmG2OeEJFbAIwxr4jIHcCtQDVQBvyfMWbF8T5XE5SqT04OPPQQLFgABw5AdX2zzXsfgNgfkIjtxEb4069TRwb3CWfYoEBOHxhFuH9wS4etlDqOZklQzUUTlDoeY6C0FPLzoaDA+rl+UynfLi1n9SoPstK9qCw7otnPXoF3l40MOC2LC8cFEWTi2LMtjKx0b267TRg61BUlUUppglLtTnU17MrKZ/mGDFavL2FjMvy8KpiC1B5gDj4CWAueJdiMJ7f+dRVP3DlQO18o1cI0QSnltDe7knf+u5Nyrz14xWwjZV8m7/75YipTT8E28km69SmmoyQR5ejGuWN9uPicaIJ9A9izB95/H7Kz4d57ITLS1SVRyj1oglLqGMrKa7n4qjw+XxDx240+uXhE7KJ6zyAwNsRWg39QJQ//I5vpN8Rit9lP6FzbtsF770G/fnDJJeDp2USFUKqN0gSl1HEYAz/9BF5eEB5ZxY4DW/nofyUs/sKf9F/8CUv8HkfifHbl7qPoPy/A3iSIXosdL2zloTh8K4iJT6HHoL306ldOp2hvOkf5Ex+ShCnozPbtMHs2/O9/v54zMhJuugkuvRQGDrTGLjyW/HxIT4f+/Zv1q1CqRWmCUqqJGGPYsi+FR58sZuWSYMS7EOO9n8I8Hwp29MdU+B/12JCwKsZfnsbwizeSszOGbz/ow/Kvg6itFaKiYOxY6NABfH2tB5eHDrVeNTXwr3/BU09ZPRjPOw+eew561vvYe8vYvRtSUuDMM10Xg3IPmqCUagHV1fDzz7B5ayW7skrYnVlGavEm1pV/TJ5jLXRYD44j5sMq6gA7z8G+8zzYfQZSEUJNhRfGWNUph2cNnj6VlBT4MH5CLaefZuMf/4CKCjj/fCthZWRYo3GcdhoMHw7h4bBnj5VEqqshIAACAyE2Fvr0sRLbgQNWc+Mvv4CfH0RFQUyMte14NblvvoEpU2D/fvjHP+CPf2ye71O1D5qglHKhmtoalu9eTklVCaE+oQR7B1NYUcjeor3sLd7L/rL97C/bz+6C3Xyx8wsKywuhLBR2D4ddo6CwIwx9CZ/uaxkcPZiQmr6kzLuBvcl9CAovJiSylMoyB6kboykv9jl0Xg9HLR4ehvKyht8ni4uDyy6DCy+0EtvB5838/MDfH+bOhXvusRJd797WqB8PPQSPPFJ/Ytu3z5ph+dNPrabJP/3J+lylDtIEpVQbUVFdwbep37Jizwq6h3ZnYNRAovyjWJm+kiVpS1i/bz3ZJdlkl2RzoOwAhjr/h2sFcvtCpT8E7Qa/fWAzUGOHygDI74JPwWD62SbRs2M4fXrbGNDHl44+PSjJ92PnTivhfPWV1ax4NBdeCO+8YzVFTptm3Vu7+GKrObJrVygrgzVrYPVq62UMdOpk1epiYuCf/wRvb1i4EJYutWqCjz8OQUHWvp98Aq+8Anv3Qm4u1NZaSfO226BXL9i50zp2+3brfN26wZAh1s/jqa6Gb7+1kmuXLsfff/du+Ppra5bosLDj798QRUXg42PVeuuzaRN88AHceqtVs3V3mqCUckPGGKprqymvLqfW1OLt4Y2n3ZOiyiLS8tNIPZCKw+6ge0h3Ogd1ZsWeFbyz4R0WbF5ASVXJoc8RhJ5hPUmISsDT7knxAR+yN/fF0+aNr6cXAZ4BxPr0JsazFx2jfLnsMqgxVVTXVuNl9+FPf4LXXrOa/A4KCIDBg2HUKKu34oABsHIl3H47rFtn7RMSAklJVgKIjLSaChcsgB9+sGpyAwZYSaGw0EpIVVXQubOVNMC6T5ef/+s5J0yAu++2andffWV9rs0GI0fC6afDkiXw9NOQmmolyAcesB4Z8Pa2Hh/YsMFqBu3Z0zrXM8/AE09YCTcoyKr9/f73VnI56MABq5k0NRXS0qyXiNXpZeBA67OCgqx169ZZ5//Pf6zy/eMfVrIXsRLzunXw5JMwf7712b16Wc2psbHWck0N5OW53yMOmqCUUodU1VSxr2QfWcVZZBRmsDF7I2v3rmVT9iZqTS0OuwNBKKkqoaiiiIKKAmpNLQAdAzpSWFFIUWURACHeIcQGxuLr8CXnQDk5mX5grySyUwER/mHEBccxOHowgzoMws/Tj7ySfL79zJ/QUOGMETaiAkNJ2RTIX+4JY91aT2JiDI88Ilx//eE1jKwseP11q0Z25pkwcaJ1kS8stBLEwoXw0ktWojkoKsq68NddN2wY3Hmn1ew4b55Vi3I4rA4fB/n4WM2ZOTlWcr3lFnj+eauZMiDAau4UsUYzKSg4/LsNDrZqacXFv67z8rLuC2ZkWMdfcw0sXgybN1uJMyDAKtf+/da9wt//3orziius4z7/HL7/3uoks307nH023Hef9T0cTG65ubBjh1WO7GyorLReIlYC9va2mlhHjfr1e92xw/qDwOGwvoeoKCuOL76wasDDhln3GidNssrVXDRBKaVOWkllCaszV7N893JS9qcQ7B1MmE8YNrGRWZRJelE6ZVVlhPuGE+ZjtYPlleWRU5rDttxt7Cncc5wzYDVP7h2CV0wKcREdiAuOo0tQFzoHdSbcN5zskmwyijIoqiwixj+GTkGdiPSLxGFzYLfZCfYOpltAP779NJzcXDjrLKsGBrB1Wy3//TqPbt1rmXROGA67dYX++mv461+tmtzpp8OgQVYS+flnqznyd7+zPueg776zklp1tZUUvLysJNmtm/Wza1frQl5ba9Wofv7Z+rlvn5VgExLg5putGlV1tdU0+tRTVoI62GNz8mQrHrCSxbnnWrU0gMREGDcO3nrL+ryYGCsJ5ecfZWzKeoSGWk2qO3ZYNdX69O0Lp5xilXf3biuhDRoEI0ZYz++lpVkdbNLSYNUqq5baGJqglFIuk1uay/qs9VTXVhPsHUygVyBlVWXkluaSW5pLRU0FVTVVlFeXs6dwD6n5qaQeSGV3wW7yyvIOfU6oTygBngHsLd5LZU1lveeK8I2gc1Bn/Dz98HP4kVuay6acTZRWlQJgExsd/DvQJ7wPCZEJ9I3oy678XfyY+SPJ2cn0CuvFGZ3PYHin4fSP7E9sYCwiQnFlMev2riMtP40w3zAi/SIJ9w0nxDuEAK8AbNLIq/RRbNhgNQVecQWMH2/ViMrL4d13rQQSGGgltMhI6NHDalKMibGSp8NhJdKKCigpgeXLrebDTz+1mg2vucb6XH9/K9lkZlpJvVMn69zGWEnyk0+sY3/80Tq33W4l5N69Yc4cK+E2hiYopVSbVFJZQl5ZHhG+Efg4rJs/taaW3NJcskuyqamtobq2mpzSHLbkbGFTziYyizIpqSqhpLKEYO9g+kf2p19EPwRhT+EedhfsZkvuFjbu20hZdRl2sZMQlUD/yP5szd3KT3t/osZYvUT8HH5E+UeReiD18A4pddjEhq/D99Cyn8OPLsFW7S/KLwp/T3/8HH5WjTJvG9vztlNTW4O/pz/+nv7EBMTQOagzMQExVu/O4r3kluYS4h1CB/8ORPhG4LA7sIkNu9jxdfji6/DF0+5JRU0FFdUV2MRGuG84EX4RBHsH42X3wsvDi1CfUDxsjZk8/VcVFdaD4p06Ne0IKJqglFLqCDW1Newu2E0H/w6Hkh9YSXFN5hq25G5ha+5W9hbvpV94P4bEDKFnaE/yy/PJLskmpzSHA2UHyC/Pp6SqBMHqZ19QUcDugt3sKthFbmkuJZUlVNRU4O/pT5/wPvQK62V1RqksprCikMyiTHbl76KosggPmwcd/DsQ5hNGfnk+WcVZVNRUHK0Ix+Xv6c+IziMY1WUUdrGTnJPMpuxN5JbmUlpVSnl1OR38O9ArrBc9QnscKn9FTQXR/tF0De5KmG8YW3O3sj5rPemF6cRHxjO4w2AGRw9maMehOOyORv07aIJSSikXqqqpwsPmgRzjKejiymJ8Hb6HNRcaYyiuLKbG1FBraqmuraa0qpTSqlIqayoP1ZRqamvILc0lpzSHgvICKmoqKK8uZ0vOFr7b9R2bczYDEBMQQ//I/kT7R+Pr8MXL7kVGUQbb87az88BObGLDz+GHp93zsKbUgz09YwNjSc5OJrskG0EouL+AAK/GPdh2tATVqHqfiIwDnseasPB1Y8xTR2wX5/YJQClwnTHmp8acUyml2qKG1DL8PX87TJaINDgB9Aw7+thXOSU52MRGmG/DH+aqNbVkFWeRU5JDj9Ae+Hn6AVbS3Fu8ly05WxqdnI7lpBOUiNiBl4CzgXRgtYgsNMZsrrPbeKCn83UqMNP5UymlVAuK8KtnpP7jsImNmIAYYgJiDlsvIvWub2qN6XYyFEgxxvxijKkE5gKTjthnEvCOsawEgkUkuhHnVEop1U40JkF1BOo+3JDuXHei+yillFK/0Zh7UPXd6Tuyx0VD9rF2FJkGTHMuFovItkbEBhAO5DbyM9oCLaf7aS9l1XK6l8aUs95RERuToNKBTnWWY4HMk9gHAGPMLGBWI+I5jIisqa9XiLvRcrqf9lJWLad7aY5yNqaJbzXQU0TiRMQTmAosPGKfhcA1YhkGFBhj9jbinEoppdqJk65BGWOqReQO4AusbuazjTGbROQW5/ZXgEVYXcxTsLqZX9/4kJVSSrUHjXoOyhizCCsJ1V33Sp33Bri9MedohCZrLmzltJzup72UVcvpXpq8nK1yJAmllFKqeYbfVUoppRpJE5RSSqlWye0SlIiME5FtIpIiIve7Op6mIiKdRGSxiGwRkU0icpdzfaiIfCUiO5w/Q1wda1MQEbuIrBORT53L7lrOYBGZLyJbnf+2p7ljWUXkbufvbbKIvC8i3u5SThGZLSLZIpJcZ91RyyYiDzivT9tE5FzXRH3ijlLOfzp/dzeIyEciElxnW6PL6VYJqs74gOOBfsDlItLPtVE1mWrgHmNMX2AYcLuzbPcD3xhjegLfOJfdwV3AljrL7lrO54HPjTF9gIFYZXarsopIR+BOIMkY0x+r1+9U3KecbwHjjlhXb9mc/2enAvHOY152Xrfagrf4bTm/AvobYxKA7cAD0HTldKsERcPGB2yTjDF7D44Eb4wpwrqQdcQq39vO3d4GLnRJgE1IRGKB84DX66x2x3IGAiOBNwCMMZXGmHzcsKxYPYZ9RMQD8MV6YN8tymmMWQrsP2L10co2CZhrjKkwxqRiPYIztCXibKz6ymmM+dIYc3Cy+ZVYgzFAE5XT3RJUuxj7T0S6AoOAVUDUwYefnT8jXRhaU5kB3AvU1lnnjuXsBuQAbzqbM18XET/crKzGmAzgaWA3sBfrgf0vcbNyHuFoZXPna9QNwGfO901STndLUA0e+6+tEhF/YAEw3RhT6Op4mpqInA9kG2PWujqWFuABDAZmGmMGASW03Wauo3Lef5kExAExgJ+IXOXaqFzGLa9RIvJnrNsQcw6uqme3Ey6nuyWoBo/91xaJiAMrOc0xxnzoXL3v4BQmzp/ZroqviQwHJopIGlYT7Zki8h7uV06wfl/TjTGrnMvzsRKWu5X1LCDVGJNjjKkCPgROx/3KWdfRyuZ21ygRuRY4H7jS/PpgbZOU090SVEPGB2yTRESw7lVsMcY8W2fTQuBa5/trgU9aOramZIx5wBgTa4zpivXv960x5ircrJwAxpgsYI+I9HauGgtsxv3KuhsYJiK+zt/jsVj3UN2tnHUdrWwLgaki4iUicViTuf7ogviahFizqt8HTDTGlNbZ1DTlNMa41Qtr7L/twE7gz66OpwnLNQKrirwBWO98TQDCsHoJ7XD+DHV1rE1Y5tHAp873bllOIBFY4/x3/RgIcceyAo8CW4Fk4F3Ay13KCbyPdW+tCqvmcOOxygb82Xl92gaMd3X8jSxnCta9poPXpFeaspw61JFSSqlWyd2a+JRSSrkJTVBKKaVaJU1QSimlWiVNUEoppVolTVBKKaVaJU1QSimlWiVNUEoppVql/wegdKc32E3xHQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot accuracy and loss plot\n",
    "plt.subplot(211)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(history.history[\"acc\"], color=\"g\", label=\"train\")\n",
    "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(history.history[\"loss\"], color=\"g\", label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# labels\n",
    "ytest = np.argmax(Ytest_stop, axis=1)\n",
    "\n",
    "# get predictions\n",
    "Ytest_ = model.predict([Xstest_stop, Xqtest_stop])\n",
    "ytest_ = np.argmax(Ytest_, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "comic-boxing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문                  |실제값    |예측값\n",
      "---------------------------------------\n",
      "은경이 어디 ?            : 복도       복도\n",
      "필웅이 어디 ?            : 화장실      화장실\n",
      "경임이 어디 ?            : 부엌       부엌\n",
      "경임이 어디 ?            : 복도       복도\n",
      "경임이 어디 ?            : 부엌       부엌\n",
      "경임이 어디 ?            : 복도       복도\n",
      "경임이 어디 ?            : 정원       정원\n",
      "수종이 어디 ?            : 복도       복도\n",
      "경임이 어디 ?            : 사무실      사무실\n",
      "수종이 어디 ?            : 사무실      사무실\n",
      "필웅이 어디 ?            : 부엌       부엌\n",
      "필웅이 어디 ?            : 정원       정원\n",
      "수종이 어디 ?            : 사무실      사무실\n",
      "필웅이 어디 ?            : 침실       침실\n",
      "필웅이 어디 ?            : 침실       침실\n",
      "은경이 어디 ?            : 부엌       부엌\n",
      "은경이 어디 ?            : 정원       정원\n",
      "은경이 어디 ?            : 부엌       부엌\n",
      "수종이 어디 ?            : 사무실      사무실\n",
      "은경이 어디 ?            : 침실       침실\n",
      "필웅이 어디 ?            : 복도       복도\n",
      "은경이 어디 ?            : 사무실      사무실\n",
      "은경이 어디 ?            : 사무실      사무실\n",
      "경임이 어디 ?            : 복도       복도\n",
      "수종이 어디 ?            : 침실       침실\n",
      "경임이 어디 ?            : 침실       침실\n",
      "필웅이 어디 ?            : 침실       침실\n",
      "수종이 어디 ?            : 부엌       부엌\n",
      "수종이 어디 ?            : 부엌       부엌\n",
      "수종이 어디 ?            : 정원       정원\n"
     ]
    }
   ],
   "source": [
    "NUM_DISPLAY = 30\n",
    "\n",
    "print(\"{:20}|{:7}|{}\".format(\"질문\", \"실제값\", \"예측값\"))\n",
    "print(39 * \"-\")\n",
    "\n",
    "for i in range(NUM_DISPLAY):\n",
    "    question = \" \".join([idx2word_stop[x] for x in Xqtest_stop[i].tolist()])\n",
    "    label = idx2word_stop[ytest_[i]]\n",
    "    prediction = idx2word_stop[ytest_[i]]\n",
    "    print(\"{:20}: {:8} {}\".format(question, label, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-moment",
   "metadata": {},
   "source": [
    "*****\n",
    "# 루브릭 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-module",
   "metadata": {},
   "source": [
    "|평가문항|상세기준|\n",
    "|:----|:----|\n",
    "|1. 한국어의 특성에 알맞게 전처리가 진행되었다.|한국어 특성에 따른 토큰화, 임베딩을 거쳐 데이터셋이 적절히 구성되었다.|\n",
    "|2. 메모리 네트워크가 정상적으로 구현되어 학습이 안정적으로 진행되었다.|validation loss가 안정적으로 수렴하는 것을 확인하고 이를 시각화하였다.|\n",
    "|3. 메모리 네트워크를 통해 한국어 bAbI 태스크의 높은 정확도를 달성하였다.|추론 태스크의 테스트 정확도가 90% 이상 달성하였다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-sheriff",
   "metadata": {},
   "source": [
    "1. mecab은 사용하지 않았다. 왜냐하면 이번 문장들은 다소 쉬운 문장구조를 가지고 있었기 때문이다. 결과도 좋게 나온 것 같다. 불용어는 제거하지 않은 버전과 제거한 버전을 가지고 모델을 돌려보았다. 불용어를 제거한 쪽이 조금 성능이 덜 나왔다.\n",
    "\n",
    "\n",
    "2. 정확도가 안정적으로 수렴하는 것을 표로 도출하여 확인하였다.\n",
    "\n",
    "\n",
    "3. 불용어 제거하지 않은 버전과 제거한 버전 모두 정확도가 90%이상 나옴을 확인했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-telescope",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-junction",
   "metadata": {},
   "source": [
    "오랜만에 끝까지 한 노드였다. 중간에 모델 학습 부분에서 incompatible Error가 떠서 포기할까 했는데 셀의 위치를 변경시키니 돌아갔다. 분명 내가 잘못 전처리를 한 것도 아니고 규격을 다 맞췄는데, 에러가 나서 좀 당황했다. 그래도 됐으니 다행이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-pride",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
